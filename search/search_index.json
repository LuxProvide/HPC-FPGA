{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Course description","text":"<p>This course is a live workout session provided in the context of the EPICURE project, participants will explore how to program High-Performance FPGA cards using the Intel DPC++ compiler, a SYCL implementation, to facilitate heterogeneous device programming. </p> <p>Field-Programmable Gate Arrays (FPGAs) are integrated circuits that can be configured by users post-manufacturing. They consist of a network of programmable logic blocks and reconfigurable interconnects, enabling the creation of custom digital circuits. Traditionally, FPGAs have been programmed using Hardware Description Languages (HDLs) such as Verilog and VHDL. However, designing high-performance accelerators with these languages demands extensive knowledge and experience in hardware design. </p> <p>By leveraging higher-level abstractions like SYCL and OpenCL\u2014C/C++-based programming models familiar to software developers\u2014developers can generate hardware kernels through an offline compiler. This significantly simplifies FPGA programming, reducing development time compared to HDL, which typically involves more complex coding, simulation, and debugging processes. </p> <p>Following an introduction to Meluxina\u2019s FPGA cards and an overview of SYCL programming using the OneAPI software development toolkit, participants will engage in a live demonstration led by the presenter. This demonstration will feature the coding of a brief example: \u201cHow to code quantum circuits on FPGA\u201d. </p> <p>Type: Live workout session </p> <p>Start: 28/11/2024 - 9:30 am </p> <p>End: 28/11/2024 - 12:30 pm </p> <p>Location: Online </p>"},{"location":"#about-the-presenter","title":"About the presenter","text":"<ul> <li> <p>HPC software engineer in the Supercomputing Applications Team at LuxProvide</p> </li> <li> <p>Previously, research scientist at the University of Luxembourg on large-scale optimization leveraging HPC</p> </li> <li> <p>Expertise: heterogenous programming, code porting and optimization</p> </li> </ul>"},{"location":"#why-using-fpgas-as-hardware-accelerators-ha","title":"Why using FPGAs as Hardware Accelerators (HA)?","text":"<ul> <li> <p>Customizability and Reconfigurability: Unlike CPUs and GPUs, which have fixed architectures, FPGAs can be programmed to create custom hardware configurations. This allows for the optimization of specific algorithms or processes, which can be particularly beneficial for quantum simulations, where different algorithms might benefit from different hardware optimizations.</p> </li> <li> <p>Parallel Processing: FPGAs can be designed to handle parallel computations natively using pipeline parallelism.</p> </li> <li> <p>Low Latency and High Throughput: FPGAs can provide lower latency than CPUs and GPUs because they can be programmed to execute tasks without the overhead of an operating system or other software layers. This makes them ideal for real-time processing and simulations.</p> </li> <li> <p>Energy Efficiency: FPGAs can be more energy-efficient than GPUs and CPUs for certain tasks because they can be stripped down to only the necessary components required for a specific computation, reducing power consumption.</p> </li> </ul>"},{"location":"#intel-fpga-sdk-oneapi-for-fpga","title":"Intel\u00ae FPGA SDK &amp; oneAPI for FPGA","text":"<ul> <li>The Intel\u00ae FPGA Software Development Kit (SDK) provides a comprehensive set of development tools and libraries specifically designed to facilitate the design, creation, testing, and deployment of applications on Intel's FPGA hardware. The SDK includes tools for both high-level and low-level programming, including support for hardware description languages like VHDL and Verilog, as well as higher-level abstractions using OpenCL or HLS (High-Level Synthesis). This makes it easier for developers to leverage the power of FPGAs without needing deep expertise in hardware design.</li> </ul> <ul> <li>Intel\u00ae oneAPI is a unified programming model designed to simplify development across diverse computing architectures\u2014CPUs, GPUs, FPGAs, and other accelerators. The oneAPI for FPGA component specifically targets the optimization and utilization of Intel FPGAs. It allows developers to use a single, consistent programming model to target various hardware platforms, facilitating easier code reuse and system integration. oneAPI includes specialized libraries and tools that enable developers to maximize the performance of their applications on Intel FPGAs while maintaining a high level of productivity and portability.</li> </ul> <p>In this course, you will learn to:</p> <ul> <li> <p>How to use Meluxina's FPGA, i.e., Intel\u00ae FPGA</p> </li> <li> <p>How to use fundamentals for SYCL programming</p> </li> </ul> <p>Remark</p> <p>This course is not intended to be exhaustive. In addition, the described tools and features are constantly evolving. We try our best to keep it up to date. </p>"},{"location":"#who-is-the-course-for","title":"Who is the course for ?","text":"<ul> <li> <p>This course is for students, researchers, enginners wishing to discover how to use oneAPI to program FPGA. </p> </li> <li> <p>Participants should still have some experience with modern C++ (e.g., Lambdas, class deduction templates).</p> </li> </ul>"},{"location":"#about-this-course","title":"About this course","text":"<p> This course has been developed by the Supercomputing Application Services group at LuxProvide in the context of the EPICURE project.</p>"},{"location":"compile/","title":"Compiling SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"compile/#setup","title":"Setup","text":"<p>After connecting to one of Meluxina's login node, please clone first the oneAPI-sample repository with the <code>git clone --depth 1 https://github.com/oneapi-src/oneAPI-samples.git</code> in your home folder.</p> <p>Once the repository cloned, you should see the following hierarchy:</p> <pre><code>tree -d -L 2 oneAPI-samples\noneAPI-samples\n\u251c\u2500\u2500 AI-and-Analytics\n\u2502   \u251c\u2500\u2500 End-to-end-Workloads\n\u2502   \u251c\u2500\u2500 Features-and-Functionality\n\u2502   \u251c\u2500\u2500 Getting-Started-Samples\n\u2502   \u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 Jupyter\n\u251c\u2500\u2500 common\n\u2502   \u2514\u2500\u2500 stb\n\u251c\u2500\u2500 DirectProgramming\n\u2502   \u251c\u2500\u2500 C++\n\u2502   \u251c\u2500\u2500 C++SYCL\n\u2502   \u251c\u2500\u2500 C++SYCL_FPGA\n\u2502   \u2514\u2500\u2500 Fortran\n\u251c\u2500\u2500 Libraries\n\u2502   \u251c\u2500\u2500 oneCCL\n\u2502   \u251c\u2500\u2500 oneDAL\n\u2502   \u251c\u2500\u2500 oneDNN\n\u2502   \u251c\u2500\u2500 oneDPL\n\u2502   \u251c\u2500\u2500 oneMKL\n\u2502   \u2514\u2500\u2500 oneTBB\n\u251c\u2500\u2500 Publications\n\u2502   \u251c\u2500\u2500 DPC++\n\u2502   \u2514\u2500\u2500 GPU-Opt-Guide\n\u251c\u2500\u2500 RenderingToolkit\n\u2502   \u251c\u2500\u2500 GettingStarted\n\u2502   \u2514\u2500\u2500 Tutorial\n\u251c\u2500\u2500 Templates\n\u2502   \u2514\u2500\u2500 cmake\n\u2514\u2500\u2500 Tools\n    \u251c\u2500\u2500 Advisor\n    \u251c\u2500\u2500 ApplicationDebugger\n    \u251c\u2500\u2500 Benchmarks\n    \u251c\u2500\u2500 GPU-Occupancy-Calculator\n    \u251c\u2500\u2500 Migration\n    \u2514\u2500\u2500 VTuneProfiler\n</code></pre> <ul> <li>As you can see Intel provides numerous code samples and examples to help your grasping the power of the oneAPI toolkit.</li> <li>We are going to focus on <code>DirectProgramming/C++SYCL_FPGA</code>.</li> <li> <p>Create a symbolic at the root of your home directory pointing to this folder: <pre><code>ln -s oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/GettingStarted\ntree -d -L 2 GettingStarted\nGettingStarted\n\u251c\u2500\u2500 fast_recompile\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2514\u2500\u2500 src\n\u251c\u2500\u2500 fpga_compile\n\u2502   \u251c\u2500\u2500 part1_cpp\n\u2502   \u251c\u2500\u2500 part2_dpcpp_functor_usm\n\u2502   \u251c\u2500\u2500 part3_dpcpp_lambda_usm\n\u2502   \u2514\u2500\u2500 part4_dpcpp_lambda_buffers\n\u2514\u2500\u2500 fpga_template\n    \u2514\u2500\u2500 src\n</code></pre></p> </li> <li> <p>The fpga_compile folder provides basic examples to start compiling SYCL C++ code with the DPC++ compiler</p> </li> <li> <p>The fpga_recompile folder show you how to recompile quickly your code without having to rebuild the FPGA image</p> </li> <li> <p>The fpga_template is a starting template project that you can use to bootstrap a project</p> </li> </ul>"},{"location":"compile/#discovering-devices","title":"Discovering devices","text":"<p>Before targeting a specific hardware accelerator, you need to ensure that the SYCL runtime is able to detect it.</p> <p>Commands</p> <pre><code># We need a job allocation on a FPGA node\nsalloc -A &lt;ACCOUNT&gt; -t 48:00:00 -q default -p fpga -N 1\n# Load the staging environment\nmodule load env/staging/2023.1\nmodule load intel-oneapi\nmodule load 520nmx/20.4\n# Check the available devices\nsycl-ls\n</code></pre> <p>Output</p> <pre><code>[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.15.3.0.20_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7452 32-Core Processor                                3.0 [2023.15.3.0.20_160000]\n[opencl:acc:2] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0) 1.0 [2023.1]\n[opencl:acc:3] Intel(R) FPGA SDK for OpenCL(TM), p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie1) 1.0 [2023.1]\n</code></pre> <p>Note</p> <p>Note that you can use FPGA emulation on a non-FPGA node !!! Meluxina's CPU partition<pre><code>[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2023.15.3.0.20_160000]\n[opencl:cpu:1] Intel(R) OpenCL, AMD EPYC 7H12 64-Core Processor                                3.0 [2023.15.3.0.20_160000]\n</code></pre></p>"},{"location":"compile/#first-code","title":"First code","text":"<p>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src/vector_add.cpp</p> <pre><code>#include &lt;iostream&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        h.single_task&lt;VectorAddID&gt;([=]() {\n          VectorAdd(&amp;accessor_a[0], &amp;accessor_b[0], &amp;accessor_c[0], kVectSize);\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li> <p>The <code>vector_add.cpp</code> source file contains all the necessary to understand how to create a SYCL program</p> </li> <li> <p>lines 4 and 5 are the minimal headers to include in your SYCL program</p> </li> <li> <p>line 9 is a forward declaration of the kernel name</p> </li> <li> <p>lines 11-19 is a function representing our kernel. Note the absence of <code>__kernel</code>, <code>__global</code> as it exists in OpenCL</p> </li> <li> <p>lines 30-36 are pragmas defining whether you want a full compilation, a CPU emulation or the simulator</p> </li> <li> <p>line 39 is the queue creation. The queue is bounded to a device. We will discuss it later in details.</p> </li> <li> <p>lines 41-46 provides debugging information at runtime.</p> </li> <li> <p>lines 48-54 instantiates 3 vectors. <code>vec_a</code> and <code>vec_b</code> are input C++ arrays and are initialized inside the next loop. <code>vec_c</code> is an output C++ array collecting computation results between <code>vec_a</code> and <code>vec_b</code>.</p> </li> <li> <p>lines 60-62 create buffers for each vector and specify their size. The runtime copies the data to the FPGA global memory when the kernel starts</p> </li> <li> <p>line 64 submits a command group to the device queue</p> </li> <li> <p>lines 66-68 relies on accessors to infer data dependencies. \"read_only\" accessors have to wait for data to be fetched. \"no_init\" option indicates to the runtime know that the previous contents of the buffer can be discarded</p> </li> <li> <p>lines 70-73 starts a single tasks (single work-item) and call the kernel function</p> </li> <li> <p>lines 99-105 catch SYCL exceptions and terminate the execution</p> </li> </ul>"},{"location":"compile/#code-synthesis","title":"Code synthesis","text":"Left: Synthesis time (source: wikis.uni-paderborn.de) -- Right: Development workflow <ul> <li> <p>Hardware synthesis can be very long </p> </li> <li> <p>Emulation is a practical way of testing your kernels</p> </li> </ul>"},{"location":"compile/#emulation","title":"Emulation","text":"<ul> <li> <p>FPGA emulation refers to the process of using a software or hardware system to mimic the behaviour of an FPGA device. This is usually done to test, validate, and debug FPGA designs before deploying them on actual hardware. The Intel\u00ae FPGA emulator runs the code on the host cpu.</p> </li> <li> <p>Emulation is crucial to validate the functionality of your kernel design. </p> </li> <li> <p>During emulation, your are not seeking for performance.</p> </li> </ul> <p>Compile for emulation (in one step)</p> <pre><code>icpx -fsycl -fintelfpga -qactypes vector_add.cpp -o vector_add.fpga_emu\n</code></pre> <p>Intel uses the SYCL Ahead-of-time (AoT) compilation which as two steps:</p> <ol> <li> <p>The \"compile\" stage compiles the device code to an intermediate representation (SPIR-V).</p> </li> <li> <p>The \"link\" stage invokes the compiler's FPGA backend before linking.</p> </li> </ol> <p>Two-steps compilation</p> <pre><code># Compile \nicpx -fsycl -fintelfpga -qactypes -o vector_add.cpp.o -c vector_add.cpp\n# Link\nicpx -fsycl -fintelfpga -qactypes vector_add.cpp.o -o vector_add.fpga_emu\n</code></pre> <ul> <li>The compiler option <code>-qactypes</code> informs the compiler to search and include the Algorithmic C (AC) data type folder for header and libs to the AC data types libraries for Field Programmable Gate Array (FPGA) and CPU compilations.</li> <li>The Algorithmic C (AC) datatypes libraries include a numerical set of datatypes and an interface datatype for modelling channels in communicating processes in C++.</li> </ul>"},{"location":"compile/#static-reports","title":"Static reports","text":"<ul> <li> <p>During the process of compiling an FPGA hardware image with the Intel\u00ae oneAPI DPC++/C++ Compiler, various checkpoints are provided at different compilation steps. These steps include object files generation, an FPGA early image object generation, an FPGA image object generation, and finally executables generation. These checkpoints offer the ability to review errors and make modifications to the source code without needing to do a full compilation every time. </p> </li> <li> <p>When you reach the FPGA early image object checkpoint, you can examine the optimization report generated by the compiler. </p> </li> <li> <p>Upon arriving at the FPGA image object checkpoint, the compiler produces a finished FPGA image.</p> </li> </ul> <p>In order to generate the FPGA early image, you will need to add the following option:</p> <ul> <li> <p><code>-Xshardware</code></p> </li> <li> <p><code>-Xstarget=&lt;target&gt;</code> or <code>-Xsboard=&lt;board&gt;</code></p> </li> <li> <p><code>-fsycl-link=early</code></p> </li> </ul> <p>Compile for FPGA early image</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -fsycl-link=early -Xsboard=p520_hpc_m210h_g3x16 vector_add.cpp -o vector_add_report.a\n</code></pre> <ul> <li> <p>The <code>vector_add_report.a</code> is not what we target in priority. We target the reports directory <code>vector_add_report.prj</code> which has been created.</p> </li> <li> <p>You can evaluate whether the estimated kernel performance data is satisfactory by going to the /reports/ directory and examining one of the following files related to your application: <li> <p>report.html: This file can be viewed using Internet browsers of your choice</p> </li> <li>.zip: Utilize the Intel\u00ae oneAPI FPGA Reports tool,i.e., <code>fpga_report</code>"},{"location":"compile/#full-compilation","title":"Full compilation","text":"<p>This phase produces the actual FPGA bitstream, i.e., a file containing the programming data associated with your FPGA chip. This file requires the target FPGA platform to be generated and executed. For FPGA programming, the Intel\u00ae oneAPI toolkit requires the Intel\u00ae Quartus\u00ae Prime software to generate this bitstream.</p> <p>Full hardware compilation</p> <pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE vector_add.cpp -o vector_add_report.fpga\n</code></pre> <ul> <li> <p>The compilation will take several hours. Therefore, we strongly advise you to verify your code through emulation first.</p> </li> <li> <p>You can also use the <code>-Xsfast-compile</code> option which offers a faster compile time but reduce the performance of the final FPGA image.</p> </li> </ul>"},{"location":"compile/#fast-recompilation","title":"Fast recompilation","text":"<ul> <li> <p>At first glance having a single source file is not necessarily a good idea when host and device compilation differs so much</p> </li> <li> <p>However, there is two different strategies to deal with it:</p> </li> <li> <p>Use a single source file and add the <code>-reuse-exe</code></p> </li> <li> <p>Separate host and device code compilation in your FPGA project</p> </li> <li> <p>This is up to you to choose the method that suits you the most</p> </li> </ul> <p>Using the <code>-reuse-exe</code> option</p> <p><pre><code>icpx -fsycl -fintelfpga -qactypes -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -DFPGA_HARDWARE -reuse-exe=vector_add.fpga vector_add.cpp -o vector_add.fpga\n</code></pre> If only the host code changed since the previous compilation, providing the <code>-reuse-exe=image</code> flag to <code>icpx</code> instructs the compiler to extract the compiled FPGA binary from the existing executable and package it into the new executable, saving the device compilation time.</p> <p>Question</p> <ul> <li>What happens if the vector_add.fpga is missing ?</li> </ul> <p>Separating host and device code</p> <p>Go to the <code>GettingStarted/fpga_recompile</code> folder. It provides an example of separate host and device code The process is similar as the compilation process for OpenCL except that a single tool is used, i.e., <code>icpx</code></p> <ol> <li>Compile the host code: <pre><code>icpx -fsycl -fintelfpga -DFPGA_HARDWARE host.cpp -c -o host.o\n</code></pre></li> <li>Compile the FPGA image: <pre><code>icpx -fsycl -fintelfpga -Xshardware -Xsboard=p520_hpc_m210h_g3x16 -fsycl-link=image kernel.cpp -o dev_image.a\n</code></pre></li> <li>Link both: <pre><code>icpx -fsycl -fintelfpga host.o dev_image.a -o fast_recompile.fpga\n</code></pre></li> </ol>"},{"location":"dpcpp/","title":"Introduction to FPGA programming with Intel\u00ae oneAPI","text":"<p>Intel\u00ae oneAPI is a software development toolkit from Intel designed to simplify the process of developing high-performance applications for various types of computing architecture. It aims to provide a unified and simplified programming model for CPUs, GPUs, FPGAs, and other types of hardware, such as AI accelerators, allowing developers to use a single codebase for multiple platforms.</p> <p>One of the main components of oneAPI is the Data Parallel C++ (DPC++), an open, standards-based language built upon the ISO C++ and SYCL standards. DPC++ extends C++ with features like parallel programming constructs and heterogeneous computing support, providing developers with the flexibility to write code for different types of hardware with relative ease.</p> <p>In addition to DPC++, oneAPI includes a range of libraries designed to optimize specific types of tasks, such as machine learning, linear algebra, and deep learning. These include oneDNN for deep neural networks, oneMKL for math kernel library, and oneDAL for data analytics, among others.</p> <p>It's important to note that Intel oneAPI is part of Intel's broader strategy towards open, standards-based, cross-architecture programming, which is intended to reduce the complexity of application development and help developers leverage the capabilities of different types of hardware more efficiently and effectively.</p> <p>In this documentation, you will explore how to:</p> <ul> <li>Use the DPC++ compiler to create executable for Intel FPGA hardware</li> <li>Discover the SYCL C++ abstraction layer</li> <li>How to move data from and to FPGA hardware</li> <li>Optimize FPGA workflows</li> </ul> <p>In order to get an overview of FPGA computing for the HPC ecosystem, please refer to the following slides.</p>"},{"location":"dpcpp/#what-is-the-intel-oneapi-dpc-compiler","title":"What is the Intel\u00ae oneAPI DPC++ compiler","text":"<p>In heterogenous computing, accelerator devices support the host processor by executing specific portion of code more efficiently. In this context, the Intel\u00ae oneAPI toolkit supports two different approaches for heterogeous computing:</p> <p>1. Data Parallel C++ with SYCL</p> <p>SYCL (Specification for Unified Cross-platform C++) provides a higher-level model for writing standard ISO C++ code that is both performance-oriented and portable across various hardware, including CPUs, GPUs and FPGAs It enables the use of standard C++ with extensions to leverage parallel hardware. Host and kernel code share the same source file. The DPC++ compiler is adding SYCL support on top of the LLVM C++ compiler. DPC++ is distributed with the Intel\u00ae oneAPI toolkit.</p> <p>2. OpenMP for C, C++, and Fortran </p> <p>For more than two decades, OpenMP has stood as a standard programming language, with Intel implementing its 5<sup>th</sup> version. The Intel oneAPI C++ Compiler, which includes support for OpenMP offloading, can be found in the Intel oneAPI Base Toolkit, Intel oneAPI HPC Toolkit, and Intel oneAPI IoT Toolkit. Both the Intel\u00ae Fortran Compiler Classic and the Intel\u00ae Fortran Compiler equipped with OpenMP offload support are accessible through the Intel oneAPI HPC Toolkit. </p> <p>Note: OpenMP is not supported for FPGA devices.</p>"},{"location":"dpcpp/#dpc-is-one-of-the-existing-sycl-implementations","title":"DPC++ is one of the existing SYCL implementations","text":"<ul> <li>Data Parallel C++ is the oneAPI's Implementation of SYCL.</li> </ul> <p>ComputeCpp (codeplay)</p> <p>Support for ComputeCpp will no longer be provided from September 1<sup>st</sup> 2023 (see announce)</p>"},{"location":"dpcpp/#key-features-and-components","title":"Key Features and Components","text":"<ul> <li>Heterogeneous Support: Enables coding for various types of processors within the same program.</li> <li>Performance Optimization: It offers various optimization techniques to ensure code runs as efficiently as possible.</li> <li>Standard Compliance: Aligns with the latest C++ standards, along with the SYCL standard.</li> <li>Debugging and Analysis Tools: Integrates with tools that assist in debugging and analyzing code.</li> <li>Integration with IDEs: Compatible with popular Integrated Development Environments to facilitate a seamless coding experience.</li> <li>Open Source and Community Driven: This promotes collaboration and ensures that the technology stays up to date with industry needs.</li> </ul>"},{"location":"dpcpp/#sycl","title":"SYCL","text":"(source: https://www.khronos.org) <ul> <li> <p>SYCL  abstractions to enable heterogeneous device programming</p> </li> <li> <p>SYCL's stratregy  is to allow different heterogenous devices, e.g., CPUs, GPUs, FPGAs, to be used simultaneously</p> </li> <li> <p>Generic programming with templates and lambda functions</p> </li> </ul>"},{"location":"dpcpp/#intel-oneapi-fpga","title":"Intel\u00ae oneAPI FPGA","text":"<p>SYCL offers APIs and abstractions, but FPGA cards are unique to each vendor, and even within the same vendor, FPGA cards may have diverse capabilities. DPC++ targets Intel\u00ae FPGA cards specifically and extends SYCL's functions. This allows it to leverage the strength of FPGA, all the while maintaining as much generalizability and portability as possible.</p>"},{"location":"dpcpp/#references","title":"References","text":"<ul> <li>Data Parallel C++: Mastering DPC++ for Programming of Heterogeneous Systems using C++ and SYCL</li> <li>Intel\u00ae oneAPI DPC++/C++ Compiler </li> <li>SYCL official documentation</li> </ul>"},{"location":"intro/","title":"Introduction to FPGA computing for the HPC ecosystem","text":""},{"location":"intro/#field-programmable-gate-array-fpga","title":"Field Programmable Gate Array (FPGA)","text":"<p> An FPGA (Field-Programmable Gate Array) is an integrated circuit designed to be configured by the user after manufacturing. It consists of an array of programmable logic blocks and a hierarchy of reconfigurable interconnects, allowing users to create custom digital circuits. FPGAs are known for their flexibility, enabling rapid prototyping and implementation of complex functions in hardware, making them suitable for applications in telecommunications, automotive, aerospace, and various other fields where custom and High-Performance Computing is needed.</p> <ul> <li>FPGAs are a cheaper off-the-shelf alternative</li> <li>Grid of configurable logic composed of:<ul> <li>Logic Element (LEs) or Adaptive Logic Modules (ALMs)</li> <li>Programmable switches</li> <li>Digital Signal Processing blocks</li> <li>RAM blocks</li> <li>etc \u2026</li> </ul> </li> </ul>"},{"location":"intro/#applications","title":"Applications","text":""},{"location":"intro/#fpga-vendors","title":"FPGA vendors","text":"<ul> <li> <p>Two major FPGA vendors:</p> <ul> <li>Intel Altera</li> <li>Xillinx AMD</li> </ul> </li> <li> <p>Intel acquired Altera in 2015</p> </li> <li> <p>Xillinx is solely focusing on the FPGA market while Intel is a sum of many parts</p> </li> <li> <p>Both profiles are very interesting for heterogeneous computing</p> </li> <li> <p>Among the others:</p> <ul> <li>Lattice Semiconductor</li> <li>QuickLogic</li> <li>Microchip Technology</li> <li>Achronix</li> <li>Efinix</li> </ul> </li> </ul>"},{"location":"intro/#fpga-cards","title":"FPGA cards","text":"<p>FPGA Development Boards and HPC (High-Performance Computing) FPGA Cards serve different purposes and have distinct characteristics:</p> <ol> <li> <p>FPGA Development Boards are primarily designed for learning, prototyping, and small-scale projects. They typically feature user-friendly interfaces, a variety of I/O options, and often include additional components like sensors, buttons, and displays. These boards are intended for engineers, students, and hobbyists to develop and test FPGA-based designs.</p> </li> <li> <p>HPC FPGA Cards, on the other hand, are specialized for High-Performance Computing tasks. These cards are optimized for integration into data centers and High-Performance Computing environments. They focus on maximizing computational power, energy efficiency, and data throughput. HPC FPGA cards are usually designed to be mounted in servers or workstations, and they often support advanced features like high-speed memory interfaces and network connectivity.</p> </li> </ol> <p>Difference Between FPGA Development Boards and HPC FPGA Cards</p> FPGA Development BoardsHPC FPGA Cards <ul> <li>Purpose: Primarily used for prototyping, learning, and development purposes.</li> <li>Design: These boards typically come with various interfaces (like HDMI, USB, Ethernet) and peripherals (like buttons, LEDs, and sensors) to facilitate easy testing and development.</li> <li>Flexibility: They offer a broad range of input/output options to support diverse projects and experiments.</li> <li>Cost: Generally more affordable than HPC FPGA cards due to their focus on versatility and accessibility.</li> <li>Target Audience: Suitable for students, hobbyists, and engineers working on initial project phases or small-scale applications.</li> <li>Programming languages: VHDL, Verilog, System Verilog.</li> <li>Specifications: <ul> <li>Logic Cells: 33,280 </li> <li>Block RAM: 1,800 Kbits</li> <li>External memory: None</li> </ul> </li> </ul> <p></p> <ul> <li>Purpose: Designed for High-Performance Computing (HPC) applications, focusing on accelerating compute-intensive tasks.</li> <li>Design: Typically more powerful, with higher logic capacity, memory, and bandwidth capabilities. They often come with specialized cooling solutions and are designed to be mounted in server racks.</li> <li>Performance: Optimized for tasks such as data center operations, machine learning, financial modeling, and large-scale scientific computations.</li> <li>Cost: Generally more expensive due to their advanced features and high-performance capabilities.</li> <li>Target Audience: Aimed at professionals in industries requiring significant computational power, such as data scientists, researchers, and engineers in High-Performance Computing sectors.</li> <li>Programming languages: HLS: C/C++, python --  HDL: VHDL, Verilog, System Verilog.</li> <li>Specifications:<ul> <li>Logic Cells: 2,073,000 </li> <li>Block RAM: 239.5 Mb</li> <li>External memory: 16GB HBM2</li> </ul> </li> </ul> <p></p>"},{"location":"intro/#hdl-vs-hls","title":"HDL vs HLS","text":"<ul> <li> <p>Hardware Description Language require a more detailed specification of the hardware, providing a gate-level or Register Transfer Level (RTL) description. They require knowledge of the specific hardware constructs, like registers, flip-flops, etc. </p> <ul> <li>Productivity: typically takes more time as developers have to manually describe the low-level hardware details. This can result in more control and optimization but is generally more time-consuming.</li> <li>Flexibility &amp; Optimization: allow developers to describe hardware at a more granular level, there is usually greater opportunity for manual optimization of the design.</li> <li>Learning curve require a deeper understanding of hardware concepts. Thus, there's a steeper learning curve, but it can provide more expertise in hardware design.</li> <li>Use cases is used for more traditional hardware design, where control over implementation details and optimizations is critical.</li> </ul> </li> <li> <p>High-Level Synthesis allows designers to describe hardware using high-level programming languages like C, C++, or SystemC. This means that HLS works at a higher level of abstraction, where developers can describe algorithms or logic without specifying the exact hardware details speeding up design. C/C++ code/kernels are translated to HDL using an offline compiler.  </p> <ul> <li>Productivity: offer faster development time since engineers can write code using familiar programming paradigms. Automated synthesis tools then translate the high-level code into RTL, allowing quicker prototyping.</li> <li>Flexibility &amp; Optimization:  can accelerate development but it often provides less control over the final hardware implementation. This might result in less efficient utilization of FPGA resources or higher latency compared to hand-crafted RTL code.</li> <li>Learning curve has a lower learning curve for software engineers or those familiar with C/C++. This makes it more accessible to developers who might not have a hardware background.</li> <li>Use cases is often preferred for algorithm development, data flow designs, and when a software prototype exists that needs to be converted into hardware.</li> </ul> </li> </ul>"},{"location":"intro/#hls-and-kernel-based-programming-from-opencl-to-sycl","title":"HLS and Kernel-based programming: from OpenCL to SYCL","text":""},{"location":"intro/#opencl-a-low-level-api","title":"OpenCL: a Low-level API","text":"<ul> <li> <p>OpenCL is widely used throughout the industry</p> </li> <li> <p>Kernel-based programming </p> </li> <li> <p>Many silicon vendors ship OpenCL with their processors, including GPUs, DSPs and FPGAs</p> </li> </ul> <p></p>"},{"location":"intro/#sycl-a-high-level-c-abstraction","title":"SYCL: a High-level C++ abstraction","text":"<ul> <li> <p>Full coverage for all OpenCL features</p> </li> <li> <p>Interop to enable existing OpenCL code with SYCL</p> </li> <li> <p>Single-source compilation</p> </li> <li> <p>Automatic scheduling of data movement</p> </li> </ul>"},{"location":"intro/#architecture-of-an-fpga","title":"Architecture of an FPGA","text":"<ul> <li> <p>The differences between Instruction Set Architecture (ISA) for CPUs and Spatial Architecture for FPGAs lie in how they process instructions and handle computation:</p> </li> <li> <p>ISA for CPUs: Sequential, control-flow-oriented, with a fixed architecture using a predefined set of instructions. Suitable for general-purpose tasks.</p> </li> <li> <p>Spatial Architecture for FPGAs: Parallel, data-flow-oriented, with a customizable architecture that can be tailored for specific high-performance tasks. Suitable for specialized, parallelizable workloads.</p> </li> </ul> <p>Difference between Instruction Set architecture and Spatial architecture</p> Instruction Set ArchitectureSpatial Architecture <ul> <li>Made for general-purpose computation: hardware is constantly reused </li> <li>Workflow constrained by a set of pre-defined units (Control Units, ALUs, registers)</li> <li>Data/Register size are fixed</li> <li>Different instruction executed in each clock cycle : temporal execution </li> </ul> <ul> <li>Keep only what it needs -- the hardware can be reconfigured</li> <li>Specialize everything by unrolling the hardware: spatial execution</li> <li>Each operation uses a different hardware region</li> <li>The design can take more space than the FPGA offers </li> </ul> <p> </p> <ul> <li> <p>The most obvious source of parallelism for FPGA is pipelining by inserting registers to store each operation output and keep all hardware unit busy. </p> </li> <li> <p>Pipelining parallelism has therefore many stages. </p> </li> <li> <p>If you don't have enough work to fill the pipeline, then the efficiency is very low.</p> </li> <li> <p>The authors of the DPC++ book have illustrated it perfectly in Chapter 17.</p> </li> </ul> <p>Vectorization</p> <p>Vectorization is also possible but is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"intro/#fpga-parallelism-pipelining","title":"FPGA parallelism: pipelining","text":"<p>Pipelining (see FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits)</p> <ul> <li>Pipelining is a design technique used in synchronous digital circuits to increase maximum frequency (fMAX).</li> <li>This technique involves adding registers to the critical path, reducing the amount of logic between each register.</li> <li>Reducing logic between registers decreases execution time, enabling an increase in fMAX.</li> <li>The critical path is the path between two consecutive registers that has the highest latency, meaning it\u2019s where operations take the longest to complete.</li> <li>Pipelining is especially effective for processing a stream of data.</li> <li>In a pipelined circuit, different stages can process different data inputs within the same clock cycle.</li> <li>This design improves data processing throughput.</li> </ul> <p></p> <p>Maximum Frequency (fMAX)</p> <ul> <li>The fMAX of a digital circuit is its highest possible clock frequency, determining the maximum rate for updating register outputs. </li> <li>This speed is constrained by the physical propagation delay of the signal across the combinational logic between consecutive register stages.</li> <li>The delay is affected by the complexity of the combinational logic in the path, and the path with the greatest number of logic elements and highest delay sets the speed limit for the entire circuit, often known as the critical path. </li> <li>The fMAX is the reciprocal of this critical path delay, and having a high fMAX is desirable as it leads to better performance when there are no other restrictions. </li> </ul> <p>Occupancy</p> <ul> <li>The occupancy of a datapath at a specific moment signifies the fraction of the datapath filled with valid data.</li> <li>When looking at a circuit's execution of a program, the occupancy is the mean value from the beginning to the end of the program's run.</li> <li>Parts of the datapath that are unoccupied are commonly called bubbles, akin to a CPU's no-operation (no-ops) instructions, which don't influence the final output.</li> <li>Minimizing these bubbles leads to greater occupancy. If there are no other hindrances, optimizing the occupancy of the datapath will boost throughput.  Occupancy: \u2156=40% </li> </ul>"},{"location":"intro/#meluxina-bittware-520n-mx-fpgas","title":"MeluXina Bittware 520N-MX FPGAs","text":"<p>Each of the 20 MeluXina FPGA compute nodes comprise two BittWare 520N-MX FPGAs based on the Intel Stratix 10 FPGA chip. Designed for compute acceleration, the 520N-MX are PCIe boards featuring Intel\u2019s Stratix 10 MX2100 FPGA with integrated HBM2 memory. The size and speed of HBM2 (16GB at up to 512GB/s) enables acceleration of memory-bound applications. Programming with high abstraction C, C++, and OpenCLis possible through an specialized board support package (BSP) for the Intel OpenCL SDK. For more details see the dedicated BittWare product page.</p> <p> <p>Intel Stratix 520N-MX Block Diagram. </p> <p>The Bittware 520N-MX cards have the following specifications:</p> <ol> <li>FPGA: Intel Stratix 10 MX with MX2100 in an F2597 package, 16GBytes on-chip High Bandwidth Memory (HBM2) DRAM, 410 GB/s (speed grade 2).</li> <li>Host interface:    x16 Gen3 interface direct to FPGA, connected to PCIe hard IP.</li> <li>Board Management Controller<ul> <li>FPGA configuration and control</li> <li>Voltage, current, temperature monitoring</li> <li>Low bandwidth BMC-FPGA comms with SPI link</li> </ul> </li> <li>Development Tools<ul> <li>Application development: supported design flows - Intel FPGA OpenCL SDK, Intel High-Level Synthesis (C/C++) &amp; Quartus Prime Pro (HDL, Verilog, VHDL, etc.)</li> <li>FPGA development BIST - Built-In Self-Test with source code (pinout, gateware, PCIe driver &amp; host test application)</li> </ul> </li> </ol> <p>The FPGA cards are not directly connected to the MeluXina ethernet network. The FPGA compute nodes are linked into the high-speed (infiniband) fabric, and the host code can communicate over this network for distributed/parallel applications.</p> <p>More details on FPGA can be found in the presentation below:</p> <p></p>"},{"location":"oneapi_quantum/","title":"Developing Quantum Circuit","text":"<ul> <li>We will develop a simple and famous quantum circuit, i.e., Bernstein-Varizani to search an hidden element</li> <li>We will need the two Pauli H and Z gates </li> <li>We will also need a function to measure (sample) from the distribution obtained using the state vector amplitudes</li> </ul>"},{"location":"oneapi_quantum/#setup","title":"Setup","text":"<ul> <li>We need first to obtain an interactive job on the fpga partition and load some modules</li> </ul> <p>Commands</p> <pre><code># Please log out and reconnect to Meluxina using X11 forwarding\n# ssh -X ... (-X option to enables X11 forwarding)\nsrun -A &lt;ACCOUNT&gt; -t 02:00:00 -q default -p fpga -N1  --forward-x  --pty bash -i\nmodule load env/staging/2023.1\nmodule load git-lfs\nmodule load CMake jemalloc freeglut intel-oneapi 520nmx\n</code></pre> <p>FPGA emulation</p> <p>As the number of FPGA nodes is limited to 20, your interactive job may not start if the number of participants is larger than this number. If so, please use a different partition (e.g., cpu, largemem) to use FPGA emulation. <pre><code>srun -A &lt;ACCOUNT&gt; -t 02:00:00 -q default -p largemem -N1  --forward-x  --pty bash -i\nmodule load env/staging/2023.1\nmodule load git-lfs\nmodule load CMake jemalloc freeglut intel-fpga 520nmx\n</code></pre></p> <ul> <li> <p>Clone the repository if not already done: <code>git lfs clone https://github.com/LuxProvide/QuantumFPGA</code></p> </li> <li> <p>Create a symbolic link to the project <code>ln -s QuantumFPGA/code/FQSim</code> and <code>cd FQSim</code></p> </li> <li> <p>The project contains the following files:</p> </li> </ul> <pre><code>$&gt;tree -L 2\n.\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 fpga_image\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 quantum.fpga\n\u251c\u2500\u2500 include\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 blochSphere.hpp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kernels.hpp\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 bernstein-vazirani.cpp\n    \u251c\u2500\u2500 blochSphere.cpp\n    \u251c\u2500\u2500 kernels.cpp\n    \u251c\u2500\u2500 test_h_gate.cpp\n    \u251c\u2500\u2500 test_rxryrz.cpp\n    \u2514\u2500\u2500 test_z_gate.cpp\n</code></pre> <ul> <li>fpga_image : contains the fpga image build prior to the workshop training to avoid waiting hardware synthesis. Indeed, the offline compiler will extract the bitstream file <code>aocx</code> and reuse it if and only if the device code did not change</li> <li>include :<ul> <li>kernel.hpp: header file containing the signature of function.</li> <li>blochSphere.hpp: header file containing the signature of function to draw an OpenGL BlockSphere.</li> </ul> </li> <li>src :  <ul> <li>bernstein-vazirani.cpp: the source file with the Bernstein-Vazirani circuit.</li> <li>blochSphere.cpp: source file containing all code to draw an OpenGL BlockSphere.</li> <li>kernel.cpp: source file containing all code for the gates.</li> <li>test_h_gate.cpp: source file for the example testing the h gate.</li> <li>test_rxryrz.cpp: test example for the 3 rotation gates rx, ry and rz.</li> <li>test_z_gate.cpp: source file for the example testing the z gate.</li> </ul> </li> </ul>"},{"location":"oneapi_quantum/#building-code","title":"Building code","text":"<ul> <li>We strongly recommend to compile and execute your code using the <code>Intel(R) FPGA Emulation Platform</code> which does not require any FPGA board on the system.</li> </ul> <p>Commands</p> <pre><code>mkdir build-emu &amp;&amp; cd build-emu\ncmake ..\nmake fpga_emu\n</code></pre> <ul> <li>Once your code runs on the <code>Intel(R) FPGA Emulation Platform</code> without errors:</li> </ul> <p>Commands</p> <pre><code>mkdir build-fpga &amp;&amp; cd build-fpga\ncmake ..\nmake fpga\n</code></pre> <p>Using Direct Memory Access (DMA)</p> <ul> <li>DMA is enabled between host and the device if buffer data have a 64-byte alignment.</li> <li>We strongly recommend you to load our <code>jemalloc</code> module which provides such default alignment: <pre><code>module load jemalloc\nexport JEMALLOC_PRELOAD=$(jemalloc-config --libdir)/libjemalloc.so.$(jemalloc-config --revision)\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./exe\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#device-code","title":"Device code","text":"<ul> <li> <p>The highlighted code corresponds to the device code or kernel code running on the device</p> </li> <li> <p>Device code is mainly used to modify or accelerate operation related to the state vector. We are going to explain it in a few minutes.</p> </li> <li> <p>We are not going to modify this code but we will use it to create the two required gates</p> </li> </ul> kernels.cpp<pre><code>#include \"kernels.hpp\"\n#include &lt;bitset&gt;\n#include &lt;random&gt;\n#include &lt;algorithm&gt;\n#include &lt;numeric&gt;\n#include &lt;string&gt;\n\n\nstd::string toBinary(int num,int numQubits) {\n    std::string result;\n    for (int i = numQubits - 1; i &gt;= 0; i--) {\n        int bit = (num &gt;&gt; i) &amp; 1;\n        result += std::to_string(bit);\n    }\n    return result;\n}\n\nint nth_cleared(int n, int target)\n{\n    int mask = (1 &lt;&lt; target) - 1;\n    int not_mask = ~mask;\n\n    return (n &amp; mask) | ((n &amp; not_mask) &lt;&lt; 1);\n}\n\nvoid apply_gate(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates, \n                  const int target, \n                  const std::complex&lt;float&gt; A,\n                  const std::complex&lt;float&gt; B,\n                  const std::complex&lt;float&gt; C,\n                  const std::complex&lt;float&gt; D)\n{\n  queue.parallel_for&lt;class Gate&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\n              int global_id = item.get_id(0);\n              const int zero_state = nth_cleared(global_id,target);\n              const int one_state = zero_state | (1 &lt;&lt; target);\n              std::complex&lt;float&gt; zero_amp = stateVector_d[zero_state];\n              std::complex&lt;float&gt; one_amp = stateVector_d[one_state];\n              stateVector_d[zero_state] = A * zero_amp + B * one_amp;\n              stateVector_d[one_state] =  C * zero_amp + D * one_amp;\n\n          }).wait();\n}\n\nvoid h(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numQubits,const int target){\n\n    std::complex&lt;float&gt; A (1.0f,0.0f);\n    std::complex&lt;float&gt; B (1.0f,0.0f);\n    std::complex&lt;float&gt; C (1.0f,0.0f);\n    std::complex&lt;float&gt; D (-1.0f,0.0f);\n    apply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A/std::sqrt(2.0f),\n                                                                  B/std::sqrt(2.0f),\n                                                                  C/std::sqrt(2.0f),\n                                                                  D/std::sqrt(2.0f));\n\n}\nvoid z(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numQubits,\n                  const int target){\n\n    std::complex&lt;float&gt; A (1.0f,0.0f);\n    std::complex&lt;float&gt; B (0.0f,0.0f);\n    std::complex&lt;float&gt; C (0.0f,0.0f);\n    std::complex&lt;float&gt; D (-1.0f,0.0f);\n    apply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                                B,\n                                                                C,\n                                                                D);\n}\n\nvoid get_proba(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates,float *probaVector_d){\n\n  queue.parallel_for&lt;class Proba&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\n              int global_id = item.get_id(0);\n              std::complex&lt;float&gt; amp = stateVector_d[global_id];\n              probaVector_d[global_id] = std::abs(amp * amp);\n          }).wait();\n\n}\n\nvoid measure(sycl::queue &amp;queue,std::complex&lt;float&gt; *stateVector_d, int numQubits,int samples){\n    int size = std::pow(2,numQubits);\n    float *probaVector = new float[size];\n    float *probaVector_d = malloc_device&lt;float&gt;(size,queue);\n    get_proba(queue,stateVector_d,size,probaVector_d); \n    queue.memcpy(probaVector, probaVector_d, size * sizeof(float)).wait();\n    std::random_device rd;                          // Obtain a random number from hardware\n    std::mt19937 gen(rd());                         // Seed the generator\n    std::discrete_distribution&lt;&gt; dist(probaVector, probaVector + size);\n    std::vector&lt;int&gt; arr(size);\n    for(int i = 0; i &lt; samples; i++){\n        int index = dist(gen);\n        arr[index]++;\n    }\n    std::cout &lt;&lt; \"Quantum State Probabilities:\" &lt;&lt; std::endl;\n    for (size_t i = 0; i &lt; size; ++i) {\n        std::cout &lt;&lt; \"State \" &lt;&lt;toBinary(i,numQubits) &lt;&lt; \": \" &lt;&lt; arr[i] &lt;&lt; std::endl;\n    }\n    delete[] probaVector;\n\n}\n\n\nvoid rx(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numQubits,\n                  const int target,\n                  const double angle){\n\n    double angle_2 = 0.5*angle;\n    double cos = std::cos(angle_2);\n    double sin = std::sin(angle_2);\n    std::complex&lt;float&gt; A (cos,0.0f);\n    std::complex&lt;float&gt; B (0.0f,-1.0*sin);\n    std::complex&lt;float&gt; C (0.0f,-1.0*sin);\n    std::complex&lt;float&gt; D (cos,0.0f);\n    apply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                                  B,\n                                                                  C,\n                                                                  D);\n}\n\nvoid ry(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numQubits,\n                  const int target,\n                  const double angle){\n\n    double angle_2 = 0.5*angle;\n    double cos = std::cos(angle_2);\n    double sin = std::sin(angle_2);\n    std::complex&lt;float&gt; A (cos,0.0f);\n    std::complex&lt;float&gt; B (-1.0*sin,0.0f);\n    std::complex&lt;float&gt; C (sin,0.0f);\n    std::complex&lt;float&gt; D (cos,0.0f);\n    apply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                                  B,\n                                                                  C,\n                                                                  D);\n}\n\nvoid rz(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numQubits,\n                  const int target,\n                  const double angle){\n\n    double angle_2 = 0.5*angle;\n    double cos = std::cos(angle_2);\n    double sin = std::sin(angle_2);\n\n    std::complex&lt;float&gt; A (cos,-sin);\n    std::complex&lt;float&gt; B (0.0f,0.0f);\n    std::complex&lt;float&gt; C (0.0f,0.0f);\n    std::complex&lt;float&gt; D (cos,sin);\n    apply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                                  B,\n                                                                  C,\n                                                                  D);\n}\n</code></pre>"},{"location":"oneapi_quantum/#general-approach-to-apply-a-quantum-gate","title":"General approach to apply a quantum gate","text":"<ul> <li> <p>Let's consider a multiqubit register $ |\\psi \\rangle = \\sum\\limits_{k=0}^{2^N-1} \\alpha_k |k \\rangle $ which is a mixture of pure state</p> </li> <li> <p>$ |k \\rangle $ is the decimal representation of the pure state k </p> </li> <li> <p>As you can observe it, the number of pure states constituting the state vector is growing exponentially. </p> </li> <li> <p>We can take advantage of the FPGA to apply gate in parallel </p> </li> </ul> kernel code<pre><code>int nth_cleared(int n, int target)\n{\n    int mask = (1 &lt;&lt; target) - 1;\n    int not_mask = ~mask;\n\n    return (n &amp; mask) | ((n &amp; not_mask) &lt;&lt; 1);\n}\n\nvoid apply_gate(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,const unsigned int numStates, \n                  const int target, \n                  const std::complex&lt;float&gt; A,\n                  const std::complex&lt;float&gt; B,\n                  const std::complex&lt;float&gt; C,\n                  const std::complex&lt;float&gt; D)\n{\n  queue.parallel_for&lt;class Gate&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\n              int global_id = item.get_id(0);\n              const int zero_state = nth_cleared(global_id,target);\n              const int one_state = zero_state | (1 &lt;&lt; target);\n              std::complex&lt;float&gt; zero_amp = stateVector_d[zero_state];\n              std::complex&lt;float&gt; one_amp = stateVector_d[one_state];\n              stateVector_d[zero_state] = A * zero_amp + B * one_amp;\n              stateVector_d[one_state] =  C * zero_amp + D * one_amp;\n\n          }).wait();\n}\n</code></pre> <p>Example:  applying a general U gate to qubit 2:</p> <ul> <li> <p>let's consider \\(U=\\begin{pmatrix} u_1 &amp; u_2 \\\\ u_3 &amp; u_4 \\end{pmatrix}\\)</p> </li> <li> <p>Apply \\(U\\) on qubit 2 is $ I \\otimes U \\otimes I $ with $ I $ being the identity matrix</p> </li> </ul> \\[ \\begin{aligned} I \\otimes U \\otimes I|\\psi_1 \\psi_2 \\psi_3 \\rangle &amp; =  |\\psi_1 \\rangle &amp; \\otimes &amp; \\hspace{2em} U|\\psi_2 \\rangle  &amp; \\otimes &amp; | \\psi_3 \\rangle \\\\                               &amp; = \\begin{pmatrix} \\alpha_1  \\\\ \\beta_1 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} u_1 &amp; u_2 \\\\ u_3 &amp; u_4 \\end{pmatrix} \\begin{pmatrix} \\alpha_2  \\\\ \\beta_2 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} \\alpha_3  \\\\ \\beta_3 \\end{pmatrix} \\\\                               &amp; = \\begin{pmatrix} \\alpha_1  \\\\ \\beta_1 \\end{pmatrix} &amp; \\otimes &amp;  \\begin{pmatrix} u_1 \\alpha_2 + u_2 \\beta_2 \\\\ u_3 \\alpha_2  + u_4 \\beta_2 \\end{pmatrix} &amp; \\otimes &amp; \\begin{pmatrix} \\alpha_3  \\\\ \\beta_3 \\end{pmatrix} \\\\ \\end{aligned} \\] \\[ \\begin{aligned}                               \\hspace*{1cm} &amp; = \\begin{matrix}                                                ( \\alpha_1 { \\color{red}{       u_1 \\alpha_2 + u_2 \\beta_2 }}  \\alpha_3)  |000 \\rangle + \\\\                                               ( \\alpha_1 { \\color{blue}{     u_1 \\alpha_2 + u_2 \\beta_2  }}  \\beta_3  )  |001 \\rangle + \\\\                                                ( \\alpha_1 { \\color{red}{       u_3 \\alpha_2  + u_4 \\beta_2}}  \\alpha_3 )  |010 \\rangle + \\\\                                               ( \\alpha_1 { \\color{blue}{     u_3 \\alpha_2  + u_4 \\beta_2 }}  \\beta_3  )  |011 \\rangle + \\\\                                               ( \\beta_1  { \\color{green}{   u_1 \\alpha_2 + u_2 \\beta_2   }}  \\alpha_3 )  |100 \\rangle + \\\\                                               ( \\beta_1 { \\color{purple}{  u_1 \\alpha_2 + u_2 \\beta_2   }}  \\beta_3   )  |101 \\rangle + \\\\                                               ( \\beta_1 {  \\color{green}{  u_3 \\alpha_2  + u_4 \\beta_2  }}  \\alpha_3  )  |110 \\rangle + \\\\                                               ( \\beta_1{  \\color{purple}{ u_3 \\alpha_2  + u_4 \\beta_2  }}  \\beta_3    )  |111 \\rangle \\phantom{0}                                  \\end{matrix} \\end{aligned} \\] <ul> <li> <p>Nonetheless, we are not applying the tensor product \\(\\otimes\\) every time which would be inefficient </p> </li> <li> <p>Starting from specific state vector, we will apply for example a gate to the 2<sup>nd</sup> qubit like following:</p> </li> </ul> \\[ \\begin{aligned}                                \\begin{pmatrix} \\alpha_1 {\\color{red}{\\alpha_2}   } \\alpha_3 \\\\                                                \\alpha_1 { \\color{blue}{\\alpha_2} } \\beta_3  \\\\                                                \\alpha_1 { \\color{red}{\\beta_2}   } \\alpha_3  \\\\                                                \\alpha_1 { \\color{blue}{\\beta_2}  } \\beta_3   \\\\                                                 \\beta_1  {\\color{green}{\\alpha_2} } \\alpha_3  \\\\                                                \\beta_1  {\\color{purple}{\\alpha_2}}  \\beta_3   \\\\                                                \\beta_1  {\\color{green}{\\beta_2}  }  \\alpha_3   \\\\                                                \\beta_1  {\\color{purple}{\\beta_2} }  \\beta_3    \\\\                                \\end{pmatrix}                               &amp; \\rightarrow \\begin{pmatrix}                                \\alpha_1 { \\color{red}{       u_1 \\alpha_2 + u_2 \\beta_2 }}  \\alpha_3  \\\\                               \\alpha_1 { \\color{blue}{     u_1 \\alpha_2 + u_2 \\beta_2  }}  \\beta_3 \\\\                                \\alpha_1 { \\color{red}{       u_3 \\alpha_2  + u_4 \\beta_2}}  \\alpha_3  \\\\                                \\alpha_1 { \\color{blue}{     u_3 \\alpha_2  + u_4 \\beta_2 }}  \\beta_3  \\\\                                \\beta_1  { \\color{green}{   u_1 \\alpha_2 + u_2 \\beta_2   }}  \\alpha_3  \\\\                                 \\beta_1 { \\color{purple}{  u_1 \\alpha_2 + u_2 \\beta_2   }}  \\beta_3  \\\\                                \\beta_1 {  \\color{green}{  u_3 \\alpha_2  + u_4 \\beta_2  }}  \\alpha_3  \\\\                                 \\beta_1{  \\color{purple}{ u_3 \\alpha_2  + u_4 \\beta_2  }}  \\beta_3                                 \\end{pmatrix}  \\end{aligned} \\] <ul> <li> <p>As you can see in the previous example to apply a gate U with its 4 complex coefficients, we apply \\((u_1 u_2)\\) to the coefficients corresponding to basis vector with a 0 at position 2 and \\((u_3 u_4)\\) to the coefficients corresponding to basis vector with a 1 at position 2</p> </li> <li> <p>Knowing this fact, we can divide by two the search and apply the gate coefficient by only searching the 1<sup>st</sup>, 2<sup>nd</sup>, kth number where the basis vector has a 0 at the chosen position</p> </li> <li> <p>To convince you, let's continue with our previous example:</p> </li> </ul> \\[ \\begin{aligned} \\begin{matrix}  |000 \\rangle &amp; \\rightarrow \\text{1st (index 0) position with 0 at position 2}  \\\\ |001 \\rangle &amp; \\rightarrow \\text{2nd (index 1) position with 0 at position 2}  \\\\  |010 \\rangle &amp; \\\\ |011 \\rangle &amp; \\\\ |100 \\rangle &amp; \\rightarrow \\text{3rd (index 2) position with 0 at position 2} \\\\ |101 \\rangle &amp; \\rightarrow \\text{4th (index 3) position with 0 at position 2}\\\\ |110 \\rangle &amp; \\\\ |111 \\rangle &amp; \\phantom{0}   \\end{matrix} \\end{aligned} \\] <ul> <li>Starting from the indexes, we can find where we should apply \\((u_1 u_2)\\) coefficient</li> </ul> \\[ \\begin{aligned} \\begin{matrix}  00  &amp; \\rightarrow \\text{index 0. Adding 0 to position 2} &amp; \\rightarrow   |000 \\rangle   \\\\ 01  &amp; \\rightarrow \\text{index 1. Adding 0 to position 2} &amp; \\rightarrow   |001 \\rangle  \\\\  10  &amp; \\rightarrow \\text{index 2.  Adding 0 to position 2}&amp; \\rightarrow   |100 \\rangle    \\\\ 11  &amp; \\rightarrow \\text{index 3. Adding 0 to position 2} &amp; \\rightarrow   |101 \\rangle    \\\\ \\end{matrix} \\end{aligned} \\] <ul> <li> <p>To find coeffcients where \\((u_3 u_4)\\) should be applied, we only need to add 1 instead of 0 to get the corresponding basis vector</p> </li> <li> <p>Finally, we will have two functions to apply any kind of one qubit gate (except the controlled gates):</p> <ul> <li>nth_cleared: finds the Nth number where a given binary digit is set to 0. </li> <li>apply_gate:  apply a general one qubit gate by finding in parallel all pure vector with digit 2 set to 0. For each of these vector, we can easily find the one with digit set to 1 and replace the amplitudes according to what we have above </li> </ul> </li> </ul>"},{"location":"oneapi_quantum/#computing-probabilities-from-state-vector-amplitudes","title":"Computing probabilities from state vector amplitudes","text":"<ul> <li> <p>The following kernel is only used to compute the probability for each pure state vector</p> </li> <li> <p>The probability to measure \\(\u2223k\\rangle\\) is \\(|\\alpha_k|^2\\) with  $\\sum\\limits_{k=0}^{2^N-1} |\\alpha_k|^2 = 1 $</p> </li> </ul> kernel code<pre><code>queue.parallel_for&lt;class Proba&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\n      int global_id = item.get_id(0);\n            std::complex&lt;float&gt; amp = stateVector_d[global_id];\n            // Equal but numerically more stable than abs(amp)*abs(amp)\n            probaVector_d[global_id] = std::abs(amp * amp); \n        }).wait();\n</code></pre>"},{"location":"oneapi_quantum/#measuring-qubits","title":"Measuring qubits:","text":"<ul> <li> <p>Quantum State Collapse: In classical simulation, measurements typically do not affect the system being measured. However, in quantum simulations, the act of measurement causes the qubit to collapse from its superposition of states to one of the basis states (e.g., 0 or 1). This is a fundamental aspect of quantum mechanics known as wave function collapse.</p> </li> <li> <p>Probabilistic Nature: The result of measuring a qubit is probabilistic. Before measurement, a qubit in superposition has probabilities associated with its possible states. Measurement forces the qubit into one of these states, with the likelihood of each state given by its quantum amplitude squared.</p> </li> <li> <p>In simulation, measuring is a synonym of sampling </p> </li> </ul> <p>Sampling the possible outcomes</p> <ul> <li>Using the <code>void get_proba(...)</code> function, fill the body of the <code>void measure(...)</code> function</li> <li>We can use the standard library function <code>std::discrete_distribution</code> (see below)</li> <li>Add the following code in the <code>void measure(...)</code> function body <pre><code> int size = std::pow(2,numQubits);\n float *probaVector = new float[size];\n float *probaVector_d = malloc_device&lt;float&gt;(size,queue);\n get_proba(queue,stateVector_d,size,probaVector_d); \n queue.memcpy(probaVector, probaVector_d, size * sizeof(float)).wait();\n std::random_device rd;                          // Obtain a random number from hardware\n std::mt19937 gen(rd());                         // Seed the generator\n std::discrete_distribution&lt;&gt; dist(probaVector, probaVector + size);\n std::vector&lt;int&gt; arr(size);\n for(int i = 0; i &lt; samples; i++){\n     int index = dist(gen);\n     arr[index]++;\n }\n std::cout &lt;&lt; \"Quantum State Probabilities:\" &lt;&lt; std::endl;\n for (size_t i = 0; i &lt; size; ++i) {\n     std::cout &lt;&lt; \"State \" &lt;&lt;toBinary(i,numQubits) &lt;&lt; \": \" &lt;&lt; arr[i] &lt;&lt; std::endl;\n }\n delete[] probaVector;\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#implementing-the-two-pauli-h-and-z-gates","title":"Implementing the two Pauli H and Z gates","text":"<p>Pauli H gate</p> <ul> <li>The Hadamard gate puts qubits in superposition</li> <li>It transform the basis state:<ul> <li>\\(|0 \\rangle\\) to $\\frac{|0\\rangle + |1 \\rangle}{\\sqrt{2}} $</li> <li>\\(|1 \\rangle\\) to $\\frac{|0\\rangle - |1 \\rangle}{\\sqrt{2}} $</li> </ul> </li> </ul> <p> \\(\\begin{aligned} H &amp; = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 &amp; 1 \\\\1 &amp; -1 \\end{pmatrix}\\end{aligned}\\) </p> <ul> <li>We add the following code in the <code>void h(...)</code> function body <pre><code>std::complex&lt;float&gt; A (1.0f,0.0f);\nstd::complex&lt;float&gt; B (1.0f,0.0f);\nstd::complex&lt;float&gt; C (1.0f,0.0f);\nstd::complex&lt;float&gt; D (-1.0f,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A/std::sqrt(2.0f),\n                                                          B/std::sqrt(2.0f),\n                                                          C/std::sqrt(2.0f),\n                                                          D/std::sqrt(2.0f));\n</code></pre> Building and testing<pre><code>mkdir build-BV &amp;&amp; cd build-BV\ncmake -DBUILD=H ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre></li> </ul> <p>Pauli Z gate</p> <p>The Pauli-Z gate is a single-qubit rotation through \\(\\pi\\) radians around the z-axis.  \\(\\begin{aligned} Z &amp; = \\begin{pmatrix}1 &amp; 0 \\\\0 &amp; -1 \\end{pmatrix}\\end{aligned}\\) </p> <ul> <li>We add the following code in the <code>void z(...)</code> function body <pre><code>std::complex&lt;float&gt; A (1.0f,0.0f);\nstd::complex&lt;float&gt; B (0.0f,0.0f);\nstd::complex&lt;float&gt; C (0.0f,0.0f);\nstd::complex&lt;float&gt; D (-1.0f,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                            B,\n                                                            C,\n                                                            D);\n</code></pre> Building and testing<pre><code>mkdir build-BV &amp;&amp; cd build-BV\ncmake -DBUILD=Z ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre></li> </ul>"},{"location":"oneapi_quantum/#implementing-the-bernstein-varizani-algorithm","title":"Implementing the Bernstein-Varizani algorithm","text":"<p>The Bernstein-Vazirani algorithm is a quantum algorithm that highlights the superiority of quantum computers in solvingspecific problems more efficiently than classical computers. This algorithm solves the problem of determining a hiddenbinary string with minimal queries to a given function.</p>"},{"location":"oneapi_quantum/#problem-setup","title":"Problem Setup","text":"<p>You are given a black box function (oracle) that computes:</p> <ul> <li>Function: $ f(x) = a \\cdot x $</li> <li>a is a hidden string of $ n $ bits.</li> <li>x is an  \\(n\\)-bit string.</li> <li>The dot product $a \\cdot x $ is calculated as $ (a_1x_1 + a_2x_2 + \\dots + a_nx_n) $ modulo 2.</li> <li>Goal: Determine the hidden string $a $ using the fewest number of queries to \\(f\\).</li> </ul>"},{"location":"oneapi_quantum/#quantum-solution","title":"Quantum Solution,","text":"<p>The Bernstein-Vazirani algorithm uses a quantum computer to identify \\(a\\) with a single query, showing an exponential improvement in query complexity. ,</p>"},{"location":"oneapi_quantum/#steps-of-the-algorithm","title":"Steps of the Algorithm:,","text":"<ol> <li>Initialization: Start with $ n $ qubits in the state $ |0\\rangle $ and one auxiliary qubit in the state $|1\\rangle $.</li> <li>Apply Hadamard Gates: Apply Hadamard gates to all qubits, transforming each $ |0\\rangle $to $ \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}} $ and $ |1\\rangle $ to \\(\\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}\\).</li> <li>Query the Oracle: The function $ f(x) $ modifies the auxiliary qubit by $ (-1)^{f(x)} $ using a Z gate, encoding the dot product $ a \\cdot x $ in the quantum state.</li> <li>Apply Hadamard Gates Again: Applying Hadamard gates again to all qubits.</li> <li>Measurement: Measure the first $ n $ qubits to directly obtain $a $ in binary form.</li> </ol> <p> Circuit for a=\"01\" </p> <p>No CNOT gate</p> Without CNOTOr implement it ? <ul> <li>We do not have the CNOT gate.</li> <li>However, we can replace the previous circuit the following one:</li> </ul> <p></p> <pre><code>void apply_controlled_gate(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numStates,\n                  const int control,\n                  const int target,\n                  const std::complex&lt;float&gt; A,\n                  const std::complex&lt;float&gt; B,\n                  const std::complex&lt;float&gt; C,\n                  const std::complex&lt;float&gt; D)\n{\n\n  queue.parallel_for&lt;class ControlledGate&gt;(sycl::range&lt;1&gt;(numStates),[=]( sycl::item&lt;1&gt; item) {\n    int global_id = item.get_id(0);\n    const int zero_state = nth_cleared(global_id,target);\n    const int one_state = zero_state | (1 &lt;&lt; target);\n    std::complex&lt;float&gt; zero_amp = stateVector_d[zero_state];\n    std::complex&lt;float&gt; one_amp = stateVector_d[one_state];\n\n    if((zero_state &amp; (1 &lt;&lt; control)) &gt; 0){\n        stateVector_d[zero_state] = A * zero_amp + B * one_amp;\n    }\n    if((one_state &amp; (1 &lt;&lt; control)) &gt; 0){\n        stateVector_d[one_state] =  C * zero_amp + D * one_amp;\n    }\n         }).wait();\n\n}\n\n\nvoid cnot_x(sycl::queue &amp;queue, std::complex&lt;float&gt; *stateVector_d,\n                  const unsigned int numQubits,\n                  const int control,\n                  const int target){\n\n    std::complex&lt;float&gt; A (0.0f,0.0f);\n    std::complex&lt;float&gt; B (1.0f,0.0f);\n    std::complex&lt;float&gt; C (1.0f,0.0f);\n    std::complex&lt;float&gt; D (0.0f,0.0f);\n    apply_controlled_gate(queue,stateVector_d,std::pow(2,numQubits)/2,control,target,A,\n                                                                B,\n                                                                C,\n                                                                D);\n}\n</code></pre> <p>Implemetation of the SYCL version of Bernstein-Vazirani</p> <pre><code>// Sample code Bernstein-Vazirani\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include \"kernels.hpp\"\n\nusing namespace sycl;\n\n\nint main() {\n\n    bool passed = true;\n    try{\n\n       #if FPGA_SIMULATOR\n           auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n       #elif FPGA_HARDWARE\n           auto selector = sycl::ext::intel::fpga_selector_v;\n       #else  // #if FPGA_EMULATOR\n           auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n       #endif\n\n       // Create a SYCL queue\n       queue queue(selector);\n       // make sure the device supports USM host allocations\n       auto device = queue.get_device();\n\n       std::cout &lt;&lt; \"Running on device: \"\n                 &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n                 &lt;&lt; std::endl;\n\n\n\n       constexpr size_t numQubits = 7;\n       constexpr size_t numStates = 1 &lt;&lt; numQubits; // 2^n\n\n       constexpr int hidden = 101; //Hidden integer (1100101)\n\n       std::complex&lt;float&gt; *stateVector   = new std::complex&lt;float&gt;[numStates];\n       std::complex&lt;float&gt; *stateVector_d = malloc_device&lt;std::complex&lt;float&gt;&gt;(numStates,queue);\n\n       // Initial state |00...00&gt;\n       stateVector[0] = std::complex&lt;float&gt;(1.0f,0.0f);\n       queue.memcpy(stateVector_d, stateVector, numStates * sizeof(std::complex&lt;float&gt;)).wait();\n\n\n       for(int i = 0; i &lt; numQubits; i++){\n          h(queue, stateVector_d, numQubits,i);\n       }\n\n       for(int j = 0; j &lt; numQubits; j++){\n           if((hidden &amp; (1 &lt;&lt; j)) != 0){\n             z(queue, stateVector_d, numQubits, j); \n           }\n\n        }\n\n       for(int i = 0; i &lt; numQubits; i++){\n          h(queue, stateVector_d, numQubits,i);\n       }\n\n       measure(queue, stateVector_d, numQubits, 1000);\n\n    sycl::free(stateVector_d,queue);\n    }catch (exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n\n}\n</code></pre> Building and testing<pre><code>mkdir build-BV &amp;&amp; cd build-BV\ncmake -DBUILD=BV ..\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre>"},{"location":"oneapi_toolbox/","title":"Creating a Quantum toolbox","text":"<ul> <li> <p>At this point, we have shown how to implement a simple circuit with two types of gates</p> </li> <li> <p>Nonetheless, with this two gates, we can't model complex circuits</p> </li> <li> <p>We need something more universal </p> </li> </ul>"},{"location":"oneapi_toolbox/#introduction-to-the-bloch-sphere","title":"Introduction to the Bloch Sphere","text":"<ul> <li> <p>All states of a qubit \\(|\\psi\\rangle\\) can be drawn on the surface of an sphere. We can show it by going back to the definition of a single qubit: \\(|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle\\)</p> </li> <li> <p>We know that \\(\\alpha, \\beta \\in \\mathbb{C}\\) which means that \\(\\alpha=r_1e^{i\\gamma_1}\\) and \\(\\beta=r_2e^{i\\gamma_2}\\)</p> </li> <li> <p>By Observing that \\(|\\alpha|^2 + |\\beta|^2 = r_1^2 + r_2^2 = 1\\) and \\(0\\leq r_1 \\leq 1\\), \\(0\\leq r_2 \\leq 1\\), we can therefore use the following change of variable: \\(r_1=cos(\\frac{\\theta}{2})\\) and \\(r_2=sin(\\frac{\\theta}{2})\\)</p> </li> </ul> \\[ \\begin{aligned}                               |\\psi\\rangle &amp; = cos\\frac{\\theta}{2}e^{i\\gamma_1}|0\\rangle + sin\\frac{\\theta}{2}e^{i\\gamma_2}|1\\rangle \\end{aligned} \\] <ul> <li> <p>We can simplify this expression by noting that \\(|\\psi\\rangle\\) can be multiplied by any complex number or global phase \\(z\\) with \\(|z|=1\\) without changing its state</p> </li> <li> <p>Let's take \\(z=e^{-i\\gamma_1}\\) as global phase:</p> </li> </ul> \\[ \\begin{aligned}                               e^{-i\\gamma_1}|\\psi\\rangle &amp; = cos\\frac{\\theta}{2}|0\\rangle + sin\\frac{\\theta}{2}e^{i\\gamma_2-\\gamma_1}|1\\rangle                                                          &amp; = cos\\frac{\\theta}{2}|0\\rangle + sin\\frac{\\theta}{2}e^{i\\sigma}|1\\rangle \\end{aligned} \\] <ul> <li>With \\(\\theta\\) and \\(\\sigma\\), representing respectively, the polar angle and an azimuthal angle. </li> </ul> <p>  $$                               \\theta=\\frac{\\pi}{4};\\sigma=0 \\hspace{5cm} \\theta=\\frac{\\pi}{4};\\sigma=\\frac{\\pi}{4}  $$</p> <p>Warning</p> <p>If you did not already guess it, the Pauli X, Y and Z gates are rotations around their respective axis. The X gate transform \\(|0\\rangle\\) to \\(|1\\rangle\\) which corresponds to \\(\\pi\\) rotation </p> <ul> <li> <p>More general rotation operators are generated by exponentiation of the Pauli matrices, i.e., \\(e^{iAx} = cos(x)I+isin(x)A\\) </p> <ul> <li>The rotation around the \\(X\\) axis: \\(R_X(\\theta)=e^{-i\\frac{\\theta}{2}X}=\\begin{pmatrix}cos\\frac{\\theta}{2} &amp; -isin\\frac{\\theta}{2} \\\\\\\\ -isin\\frac{\\theta}{2} &amp; cos\\frac{\\theta}{2}\\end{pmatrix}\\)</li> <li>The rotation around the \\(Y\\) axis: \\(R_Y(\\theta)=e^{-i\\frac{\\theta}{2}Y}=\\begin{pmatrix}cos\\frac{\\theta}{2} &amp; -sin\\frac{\\theta}{2} \\\\\\\\ sin\\frac{\\theta}{2} &amp; cos\\frac{\\theta}{2}\\end{pmatrix}\\)</li> <li>The rotation around the \\(Z\\) axis: \\(R_Z(\\theta)=e^{-i\\frac{\\theta}{2}Z}=\\begin{pmatrix}1 &amp; 0 \\\\\\\\ 0 &amp; e^{i\\theta}\\end{pmatrix}\\)</li> </ul> </li> </ul>"},{"location":"oneapi_toolbox/#implementing-the-rotation-gates","title":"Implementing the rotation gates","text":"<p>Rotations gates</p> QuestionSolution for the \\(R_x\\) gateSolution for the \\(R_y\\) gateSolution for the \\(R_z\\) gate <ul> <li>Implement the 3 Rotation gates \\(R_x\\), \\(R_y\\) and \\(R_z\\)</li> <li>To test the rotation gates, set the variable <code>SOURCE_FILES</code> as follows <code>set(SOURCE_FILES src/test_rxryrz.cpp src/kernels.cpp src/blochSphere.cpp)</code> in the CMakeLists.txt file</li> </ul> <p>Building and the code</p> <pre><code>mkdir build-rotation-gates &amp;&amp; cd build-rotation-gates\ncmake .. -DBUILD=U -DUSER_FLAGS=\"-lGL -lGLU -lglut\"\nmake fpga\nLD_PRELOAD=${JEMALLOC_PRELOAD} ./quantum.fpga\n</code></pre> <ul> <li>You should see the following on your screen:</li> </ul> <p> OpenGL Bloch Sphere for the resulting qubit </p> <ul> <li>Add the following code in the <code>void rx(...)</code> function body <pre><code>double angle_2 = 0.5*angle;\ndouble cos = std::cos(angle_2);\ndouble sin = std::sin(angle_2);\nstd::complex&lt;float&gt; A (cos,0.0f);\nstd::complex&lt;float&gt; B (0.0f,-1.0*sin);\nstd::complex&lt;float&gt; C (0.0f,-1.0*sin);\nstd::complex&lt;float&gt; D (cos,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                          B,\n                                                          C,\n                                                          D);\n</code></pre></li> </ul> <ul> <li>Add the following code in the <code>void ry(...)</code> function body <pre><code>double angle_2 = 0.5*angle;\ndouble cos = std::cos(angle_2);\ndouble sin = std::sin(angle_2);\nstd::complex&lt;float&gt; A (cos,0.0f);\nstd::complex&lt;float&gt; B (-1.0*sin,0.0f);\nstd::complex&lt;float&gt; C (sin,0.0f);\nstd::complex&lt;float&gt; D (cos,0.0f);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                          B,\n                                                          C,\n                                                          D);\n</code></pre></li> </ul> <ul> <li>Add the following code in the <code>void rz(...)</code> function body <pre><code>double angle_2 = 0.5*angle;\ndouble cos = std::cos(angle_2);\ndouble sin = std::sin(angle_2);\nstd::complex&lt;float&gt; A (cos,-sin);\nstd::complex&lt;float&gt; B (0.0f,0.0f);\nstd::complex&lt;float&gt; C (0.0f,0.0f);\nstd::complex&lt;float&gt; D (cos,sin);\napply_gate(queue,stateVector_d,std::pow(2,numQubits)/2,target,A,\n                                                              B,\n                                                              C,\n                                                              D);\n</code></pre></li> </ul>"},{"location":"optimization/","title":"Optimizing SYCL programs for Intel\u00ae FPGA cards","text":"<p>Optimizing SYCL code for Intel FPGAs requires a combination of understanding the FPGA hardware, the SYCL programming model, and the specific compiler features provided by Intel. Here are some general guidelines to optimize Intel FPGA SYCL code.</p> <p>Compared to OpenCL, the Intel\u00ae oneAPI DPC++ compiler has enhanced features to detect possible optimizations( vectorization, static coalescing, etc ...). Nonetheless, some rules need to be followed to make sure the compiler is able to apply these optimizations. </p> <p>Optimizing your design</p> <p>As this course/workshop is only an introduction to the Intel\u00ae oneAPI for FPGA programming, we can't unfortunately provide all existing and possible optimizations. Many more optimizations can be found in the Intel official documentation.</p>"},{"location":"optimization/#loop-optimization","title":"Loop optimization","text":"<p>Loop unrolling is an optimization technique that aims to increase parallelism and, consequently, the throughput of certain computational tasks, particularly when implemented in hardware environments such as FPGAs. </p> <ol> <li> <p>Pipelining Synergy: Loop unrolling often goes hand in hand with pipelining in FPGAs. When loops are unrolled, each unrolled iteration can be pipelined, leading to even greater throughput enhancements.</p> </li> <li> <p>Resource Utilization: While loop unrolling can significantly speed up operations, it also consumes more FPGA resources, like Logic Elements (LEs) and registers, because of the duplicated hardware. Hence, there's a trade-off between speed and resource utilization.</p> </li> <li> <p>Memory Access: Unrolling loops that involve memory operations can lead to increased memory bandwidth utilization. In cases where memory bandwidth is a bottleneck, unrolling can provide substantial performance improvements.</p> </li> <li> <p>Latency &amp; Throughput: Loop unrolling doesn't necessarily reduce the latency of a single loop iteration (the time taken for one iteration to complete), but it can significantly improve the throughput (number of completed operations per unit time).</p> </li> <li> <p>Reduction in Control Logic: Unrolling can reduce the overhead associated with the loop control logic, such as incrementing the loop counter and checking the loop termination condition.</p> </li> </ol> <p>Increasing throughput with loop unrolling</p> How to unroll loopsQuestionSolution <ul> <li>Unrolling loop can be done using the <code>#pragma unroll &lt;N&gt;</code></li> <li><code>&lt;N&gt;</code> is the unroll factor</li> <li><code>#pragma unroll 1</code> : prevent a loop in your kernel from unrolling</li> <li><code>#pragma unroll</code> : let the offline compiler decide how to unroll the loop  <pre><code>handler.single_task&lt;class example&gt;([=]() {\n    #pragma unroll\n        for (int i = 0; i &lt; 10; i++) {\n            acc_data[i] += i;\n        }\n    #pragma unroll 1\n    for (int k = 0; k &lt; N; k++) {\n        #pragma unroll 5\n        for (int j = 0; j &lt; N; j++) {\n            acc_data[j] = j + k;\n        }\n    }\n});\n</code></pre></li> </ul> <ul> <li>Consider the following code that you can find at <code>oneAPI-samples/DirectProgramming/C++SYCL_FPGA/Tutorials/Features/loop_unroll</code></li> <li>Note that Intel did not consider data alignment which could impact performance</li> <li>We included <code>#include &lt;boost/align/aligned_allocator.hpp&gt;</code> to create aligned std::vector</li> <li>The following SYCL code has been already compiled for you, execute it on the FPGA nodes for several data input size and record the throughput and kernel time</li> <li>What do you observe ? <pre><code>//==============================================================\n// Copyright Intel Corporation\n//\n// SPDX-License-Identifier: MIT\n// =============================================================\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;string&gt;\n#include &lt;vector&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\nusing namespace sycl;\n\nusing aligned64_vector= std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt;;\n\n// Forward declare the kernel name in the global scope.\n// This FPGA best practice reduces name mangling in the optimization reports.\ntemplate &lt;int unroll_factor&gt; class VAdd;\n\n// This function instantiates the vector add kernel, which contains\n// a loop that adds up the two summand arrays and stores the result\n// into sum. This loop will be unrolled by the specified unroll_factor.\ntemplate &lt;int unroll_factor&gt;\nvoid VecAdd(const aligned64_vector &amp;summands1,\n            const aligned64_vector &amp;summands2, aligned64_vector &amp;sum,\n            size_t array_size) {\n\n#if FPGA_SIMULATOR\n  auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n  auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n  auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n  try {\n    queue q(selector,property::queue::enable_profiling{});\n\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    buffer buffer_summands1(summands1);\n    buffer buffer_summands2(summands2);\n    buffer buffer_sum(sum);\n\n    event e = q.submit([&amp;](handler &amp;h) {\n      accessor acc_summands1(buffer_summands1, h, read_only);\n      accessor acc_summands2(buffer_summands2, h, read_only);\n      accessor acc_sum(buffer_sum, h, write_only, no_init);\n\n      h.single_task&lt;VAdd&lt;unroll_factor&gt;&gt;([=]()\n                                         [[intel::kernel_args_restrict]] {\n        // Unroll the loop fully or partially, depending on unroll_factor\n        #pragma unroll unroll_factor\n        for (size_t i = 0; i &lt; array_size; i++) {\n          acc_sum[i] = acc_summands1[i] + acc_summands2[i];\n        }\n      });\n    });\n\n    double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();\n    double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();\n    // convert from nanoseconds to ms\n    double kernel_time = (double)(end - start) * 1e-6;\n\n    std::cout &lt;&lt; \"unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \" kernel time : \" &lt;&lt; kernel_time &lt;&lt; \" ms\\n\";\n    std::cout &lt;&lt; \"Throughput for kernel with unroll_factor \" &lt;&lt; unroll_factor\n              &lt;&lt; \": \";\n    std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(3)\n#if defined(FPGA_SIMULATOR)\n              &lt;&lt; ((double)array_size / kernel_time) / 1e3f &lt;&lt; \" MFlops\\n\";\n#else\n              &lt;&lt; ((double)array_size / kernel_time) / 1e6f &lt;&lt; \" GFlops\\n\";\n#endif\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n}\n\nint main(int argc, char *argv[]) {\n#if defined(FPGA_SIMULATOR)\n  size_t array_size = 1 &lt;&lt; 4;\n#else\n  size_t array_size = 1 &lt;&lt; 26;\n#endif\n\n  if (argc &gt; 1) {\n    std::string option(argv[1]);\n    if (option == \"-h\" || option == \"--help\") {\n      std::cout &lt;&lt; \"Usage: \\n&lt;executable&gt; &lt;data size&gt;\\n\\nFAILED\\n\";\n      return 1;\n    } else {\n      array_size = std::stoi(option);\n    }\n  }\n\n  aligned64_vector summands1(array_size);\n  aligned64_vector summands2(array_size);\n\n  aligned64_vector sum_unrollx1(array_size);\n  aligned64_vector sum_unrollx2(array_size);\n  aligned64_vector sum_unrollx4(array_size);\n  aligned64_vector sum_unrollx8(array_size);\n  aligned64_vector sum_unrollx16(array_size);\n\n  // Initialize the two summand arrays (arrays to be added to each other) to\n  // 1:N and N:1, so that the sum of all elements is N + 1\n  for (size_t i = 0; i &lt; array_size; i++) {\n    summands1[i] = static_cast&lt;float&gt;(i + 1);\n    summands2[i] = static_cast&lt;float&gt;(array_size - i);\n  }\n\n  std::cout &lt;&lt; \"Input Array Size:  \" &lt;&lt; array_size &lt;&lt; \"\\n\";\n\n  // Instantiate VecAdd kernel with different unroll factors: 1, 2, 4, 8, 16\n  // The VecAdd kernel contains a loop that adds up the two summand arrays.\n  // This loop will be unrolled by the specified unroll factor.\n  // The sum array is expected to be identical, regardless of the unroll factor.\n  VecAdd&lt;1&gt;(summands1, summands2, sum_unrollx1, array_size);\n  VecAdd&lt;2&gt;(summands1, summands2, sum_unrollx2, array_size);\n  VecAdd&lt;4&gt;(summands1, summands2, sum_unrollx4, array_size);\n  VecAdd&lt;8&gt;(summands1, summands2, sum_unrollx8, array_size);\n  VecAdd&lt;16&gt;(summands1, summands2, sum_unrollx16, array_size);\n\n  // Verify that the output data is the same for every unroll factor\n  for (size_t i = 0; i &lt; array_size; i++) {\n    if (sum_unrollx1[i] != summands1[i] + summands2[i] ||\n        sum_unrollx1[i] != sum_unrollx2[i] ||\n        sum_unrollx1[i] != sum_unrollx4[i] ||\n        sum_unrollx1[i] != sum_unrollx8[i] ||\n        sum_unrollx1[i] != sum_unrollx16[i]) {\n      std::cout &lt;&lt; \"FAILED: The results are incorrect\\n\";\n      return 1;\n    }\n  }\n  std::cout &lt;&lt; \"PASSED: The results are correct\\n\";\n  return 0;\n}\n</code></pre></li> </ul> Unroll factor kernel execution time (ms) Throughput (GFlops) 1 77 0.447 2 58 0.591 4 43 0.804 8 40 0.857 16 39 0.882 <ul> <li>Increasing the unroll factor improves throughput    </li> <li>Nonetheless, unrolling large loops should be avoided as it would require a large amount of hardware</li> </ul> <p>Recording kernel time</p> <ul> <li>In this example, we have also seen how to record kernel time.</li> <li>Using the property `property::queue::enable_profiling{}`` adds the requirement that the runtime must capture profiling information for the command groups that are submitted from the queue </li> <li>You can the capture  the start &amp; end time using the following two commands:<ul> <li><code>double start = e.get_profiling_info&lt;info::event_profiling::command_start&gt;();</code></li> <li><code>double end = e.get_profiling_info&lt;info::event_profiling::command_end&gt;();</code></li> </ul> </li> </ul> <p>Caution with nested loops</p> <ul> <li>Loop unrolling involves replicating the hardware of a loop body multiple times and reducing the trip count of a loop. Unroll loops to reduce or eliminate loop control overhead on the FPGA. </li> <li>Loop-unrolling can be used to eliminate nested-loop structures.</li> <li>However avoid unrolling the outer-loop which will lead to Resource Exhaustion and dramatically increase offline compilation</li> </ul>"},{"location":"optimization/#simd-work-items-for-nd-range-kernels","title":"SIMD Work Items for ND-Range kernels","text":"<ul> <li>ND-range kernel should use instead of classical data-parallel kernels</li> <li>The work-group size needs to be set using the attribute <code>[[sycl::reqd_work_group_size(1, 1, REQD_WG_SIZE)]]</code></li> <li>To specify the number of SIMD work_items, you will need to add the following attribute <code>[[intel::num_simd_work_items(NUM_SIMD_WORK_ITEMS)]]</code></li> <li>Note that NUM_SIMD_WORK_ITEMS should divide evenly REQD_WG_SIZE</li> <li>The supported values for NUM_SIMD_WORK_ITEMS  are 2, 4, 8, and 16</li> </ul> <p>Example</p> <pre><code>...\nh.parallel_for&lt;VectorAddID&gt;(\nsycl::nd_range&lt;1&gt;(sycl::range&lt;1&gt;(2048), sycl::range&lt;1&gt;(128)),        \n    [=](sycl::nd_item&lt;1&gt; it) \n    [[intel::num_simd_work_items(8),\n    sycl::reqd_work_group_size(1, 1, 128)]] {\n    auto gid = it.get_global_id(0);\n    accessor_c[gid] = accessor_a[gid] + accessor_b[gid];\n    });\n});\n...\n</code></pre> <ul> <li> <p>The 128 work-items are evenly distributed among 8 SIMD lanes</p> </li> <li> <p>128/8 = 16 wide vector operation</p> </li> <li> <p>The offline compiler coalesces 8 loads to optimize (reduce) the access to memory in case there are no data dependencies</p> </li> </ul>"},{"location":"optimization/#loop-coalescing","title":"Loop coalescing","text":"<p>Utilize the <code>loop_coalesce</code> attribute to instruct the Intel\u00ae oneAPI DPC++/C++ Compiler to merge nested loops into one, preserving the loop's original functionality. By coalescing loops, you can minimize the kernel's area consumption by guiding the compiler to lessen the overhead associated with loop management.</p> <p>Coalesced two loops</p> Using the loop_coalesce attribute <pre><code>[[intel::loop_coalesce(2)]]\nfor (int i = 0; i &lt; N; i++)\n   for (int j = 0; j &lt; M; j++)\n      sum[i][j] += i+j;\n</code></pre> Equivalent code <pre><code>int i = 0;\nint j = 0;\nwhile(i &lt; N){\n  sum[i][j] += i+j;\n  j++;\n  if (j == M){\n    j = 0;\n    i++;\n  }\n}\n</code></pre>"},{"location":"optimization/#memory","title":"Memory","text":""},{"location":"optimization/#static-coalescing","title":"Static coalescing","text":"<ul> <li> <p>Static coalescing is performed by the Intel\u00ae oneAPI DPC++/C++ Compiler contiguous accesses to global memory can be merged into a single wide access.</p> </li> <li> <p>For static memory coalescing to occur, your code should be structured so that the compiler can detect a linear access pattern at compile time. The initial kernel code depicted in the previous figure can leverage static memory coalescing, as all indices into buffers a and b increase with offsets recognizable during compilation.</p> </li> </ul> <p> </p> FPGA Optimization Guide for Intel\u00ae oneAPI Toolkits -- Figure 17-21"},{"location":"optimization/#data-structure-alignment","title":"Data structure alignment","text":"<p>In order to performance, structure alignment can be modified to be properly aligned. By default, the offline compiler aligns these elements based on:</p> <ul> <li>The alignment should be a power of two.</li> <li>The alignment should be a multiple of the least common multiple (LCM) of the word-widths of the structure member sizes.</li> </ul> <p>Let's take a simple but clear example to understand why alignment is so important.</p> <p></p> <ul> <li> <p>Each element of MyStruct has 12 bytes  due to padding</p> </li> <li> <p>Recall that each transaction between the user kernel design and the memory controller is 512 bits wide to enable DMA</p> </li> <li> <p>We have therefore 64/12 = 5.333 =&gt; alignment is far from optimal as the 6<sup>th</sup> element of MyStruct will be split between two 64-byte regions</p> </li> </ul> <p></p> <ul> <li> <p>Removing all padding will definitely reduce the size </p> </li> <li> <p>Padding can be removed by adding  the \u201cpacked\u201d attribute, i.e, \u201cattribute((packed))\u201d in your kernel</p> </li> <li> <p>Each element of MyStruct will have therefore 9 bytes</p> </li> <li> <p>However, 64/9 = 7.111 =&gt; we still have some elements in multiple 64-bytes region and the alignment is sub-optimal</p> </li> </ul> <p></p> <ul> <li> <p>To improve performance, align structure  such all elements belongs to a single 64-byte regions</p> </li> <li> <p>Padding can still be removed by adding  the \u201cpacked\u201d attribute, i.e, \u201cattribute((packed))\u201d </p> </li> <li> <p>Transaction size is 64 bytes, the minimum alignment which is also a multiple of the transaction size is 16 </p> </li> <li> <p>Enforce a 16-byte alignment with <code>__attribute__((aligned(16)))</code></p> </li> </ul> <p></p> <p>Removing padding and changing structure alignment</p> CodeExecution time <ul> <li> <p>The following code show the impact of changing the alignmement and padding using three scenarii:</p> <ul> <li> <p>Default alignment and padding </p> </li> <li> <p>Removing padding</p> </li> <li> <p>Changing alignment </p> </li> </ul> </li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;typeinfo&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;chrono&gt;\nusing namespace std::chrono;\n\n#define ALIGNMENT 64\n#define IT 1024\n\n\nconstexpr int kVectSize = 2048;\n\n\ntemplate&lt;typename T&gt;\nvoid test_structure( T* device,sycl::queue &amp;q, int nb_iters){\n\n      sycl::event e;\n      const sycl::property_list props = {sycl::property::buffer::use_host_ptr()};\n\n      auto start = high_resolution_clock::now();\n      sycl::buffer buffer_device{device, sycl::range(kVectSize),props};\n      e = q.submit([&amp;](sycl::handler &amp;h) {\n       sycl::accessor accessor_device{buffer_device, h, sycl::read_write};\n       h.single_task([=]() {\n       for(int it=0;it &lt; nb_iters ;it++){\n        for (int idx = 0; idx &lt; kVectSize; idx++) {\n          accessor_device[idx].C = (int)accessor_device[idx].A + accessor_device[idx].B;\n         }\n        }\n        });\n       });\n\n    sycl::host_accessor buffer_host(buffer_device);\n    auto stop = high_resolution_clock::now();\n    // convert from nanoseconds to ms\n    duration&lt;double&gt; kernel_time = stop - start;\n\n    std::cout  &lt;&lt; \" Time (\" &lt;&lt;typeid(T).name()&lt;&lt;  \") : \" &lt;&lt; kernel_time.count() &lt;&lt; \" ms\\n\";\n}\n\nint main() {\n\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector,sycl::property::queue::enable_profiling{});\n\n    auto device = q.get_device();\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    typedef struct {\n        char A;\n        int  B;\n        int  C;\n    } mystruct;\n\n    typedef struct __attribute__ ((packed)) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed;\n\n\n    typedef struct __attribute__ ((packed)) __attribute__ ((aligned(16))) {\n        char A;\n        int  B;\n        int  C;\n    } mystruct_packed_aligned;\n\n\n    mystruct* vec_a = new(std::align_val_t{ 64 }) mystruct[kVectSize];\n    mystruct_packed* vec_b = new(std::align_val_t{ 64 }) mystruct_packed[kVectSize];\n    mystruct_packed_aligned* vec_c = new(std::align_val_t{ 64 }) mystruct_packed_aligned[kVectSize];\n\n\n    for (int i = 0; i &lt; kVectSize; i++) {\n        vec_a[i].A = vec_b[i].A = vec_c[i].A = char(std::rand() % 256);\n        vec_a[i].B = vec_b[i].B = vec_c[i].B = std::rand();\n        vec_a[i].C = vec_b[i].C = vec_c[i].C = std::rand();\n    }\n\n    std::cout &lt;&lt; \"Packed with default alignment\" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    test_structure&lt;mystruct&gt;(vec_a,q,IT);\n    test_structure&lt;mystruct_packed&gt;(vec_b,q,IT);\n    test_structure&lt;mystruct_packed_aligned&gt;(vec_c,q,IT);\n\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n\n    //sycl::free(vec_a,q);\n    //sycl::free(vec_b,q);\n    //sycl::free(vec_c,q);\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> Scenario Processing time (seconds) Default alignment and padding 14.33 Removing padding 6.35 Changing alignment 0.03"},{"location":"optimization/#memory_1","title":"Memory","text":"Type Access Hardware Host memory read/write only by host DRAM Global memory (device) read/write by host and work-items FPGA DRAM (HBM, DDR,QDR) Local memory (device) read/write only by work-group RAM blocks Constant memory (device) read/write by host read only  by work-items FPGA DRAM  RAM blocks Private memory device read/write by single work-item only RAM blocks Registers <ul> <li> <p>Transfers between host memory and global device memory should leverage DMA for efficiency.</p> </li> <li> <p>Of all memory types on FPGAs, accessing device global memory is the slowest.</p> </li> <li> <p>In practice, using local device memory is advisable to reduce global memory accesses.</p> </li> </ul>"},{"location":"optimization/#local-memory-in-nd-range-kernels","title":"Local memory in ND-Range kernels","text":"<ul> <li>You can improve memory access by using local and private memory.</li> <li>When you define a private array, group local memory, or a local accessor, the Intel\u00ae oneAPI DPC++/C++ Compiler generates kernel memory in the hardware. This kernel memory is often termed on-chip memory since it originates from memory resources, like RAM blocks, present on the FPGA.</li> <li>Local or private memory is a fast memory that should be favored when resources allow.</li> </ul> <p>Private memory</p> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n// Create an accessor for device global memory from buffer buff\naccessor acc(buff, h, write_only);\ncgh.single_task([=]() {\n     // Declare a private array\n     int T[N];\n     // Write to private memory\n     for (int i = 0; i &lt; N; i++)\n        T[i] = i;\n     // Read from private memory and write to global memory through the accessor\n     for (int i = 0; i &lt; N; i+=2)\n        acc[i] = T[i] + T[i+1];\n     });\n}); \n...\n</code></pre> <ul> <li> <p>Two ways to define local memory for work-groups:</p> <ul> <li> <p>If you have function scope local data using group_local_memory_for_overwrite, the Intel\u00ae oneAPI DPC++/C++ Compiler statically sizes the local data that you define within a function body at compilation time.</p> </li> <li> <p>For accessors in the local space, the host assigns their memory sizes dynamically at runtime. However, the compiler must set these physical memory sizes at compilation time. By default, that size is 16 kB. </p> </li> </ul> </li> </ul> <p>Local memory</p> Using sycl::local_accessorUsing <code>group_local_memory</code> functions <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n    h.parallel_for(\n        nd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item)\n        [[intel::max_work_group_size(1, 1, 16)]] { \n        int local_id = item.get_local_id();\n        sycl::local_accessor&lt;float,2&gt; lmem{B, h};\n        lmem[local_id] = local_id++;\n        });\n    });\n... \n</code></pre> <pre><code>...\nq.submit([&amp;](handler &amp;h) {\n    h.parallel_for(\n        nd_range&lt;1&gt;(range&lt;1&gt;(256), range&lt;1&gt;(16)), [=](nd_item&lt;1&gt; item) {\n        int local_id = item.get_local_id();\n        auto ptr = group_local_memory_for_overwrite&lt;int[16]&gt;(item.get_group());\n        auto&amp; ref = *ptr;\n        ref[local_id] = local_id++ ;\n        });\n    });\n... \n</code></pre> <ul> <li>The ND-Range kernel has 16 workgroups with 16 work items for each group.</li> <li>A group-local variable (int[16]) is created for each group and shared through a multi_ptr to all work-items of the same group</li> </ul> <ul> <li>Use sycl::local_accessor when data needs to be retained and shared within the same work-group and when precise control over data access and synchronization is necessary. Opt for group_local_memory_for_overwrite when you need temporary storage that can be efficiently reused across multiple work-groups, without persistence or heavy synchronization overhead.</li> </ul>"},{"location":"optimization/#kernel-memory-system","title":"Kernel Memory System","text":"<ul> <li> <p>Before diving into the different settings, we need to introduce some definitions:</p> <ul> <li> <p>Port: a memory port serves as a physical access point to memory, connecting to one or more load-store units (LSUs) within the datapath. An LSU can interface with multiple ports, and a port can be linked to multiple LSUs.</p> </li> <li> <p>Bank: a memory bank is a division within the kernel memory system, holding a unique subset of the kernel's data. All data is distributed across these banks, and every memory system has at least one bank.</p> </li> <li> <p>Replicate: a memory bank replicate is a copy of the data within a memory bank, with each replicate containing identical data. Replicates are independently accessible, and every memory bank includes at least one replicate.</p> </li> <li> <p>Private Copy: a private copy is a version of data within a replicate, created for nested loops to support concurrent outer loop iterations. Each outer loop iteration has its own private copy, allowing different data per iteration.</p> </li> </ul> </li> </ul>"},{"location":"optimization/#settings-memory-banks","title":"Settings memory banks","text":"<ul> <li>Local data can be stored  in separate  local memory banks for parallel memory accesses</li> <li>Number of banks of a local memory can be adjusted (e.g., to increase the parallel access) </li> <li>Add the following attributes <code>[[intel::numbanks(#NB), intel::bankwidth(#BW)]]</code>:  <ul> <li><code>#NB</code> : number of banks </li> <li><code>#BW</code> : bankwidth to be considered in bytes</li> </ul> </li> <li>Ex: <code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4]</code>; </li> <li>All rows accessible in parallel with numbanks(8) </li> <li>Different configurations patterns can be adopted </li> </ul>  [[intel::numbanks(8), intel::bankwidth(16)]] lmem[8][4]  (source: Intel)   <p>Masking the last index</p> <ul> <li>Intel's documentation states that \"To enable parallel access, you must mask the dynamic access on the lower array index\" <pre><code>[[intel::numbanks(8), intel::bankwidth(16)]] int lmem[8][4];\n#pragma unroll\nfor (int i = 0; i &lt; 4; i+=2) {\n    lmem[i][x &amp; 0x3] = ...;\n} \n</code></pre></li> </ul>"},{"location":"optimization/#local-memory-replication","title":"Local memory replication","text":"<p>Example</p> <p> <pre><code>[[intel::fpga_memory,\nintel::singlepump,\nintel::max_replicates(3)]] int lmem[16]; \nlmem[waddr] = lmem[raddr] +\n              lmem[raddr + 1] +\n              lmem[raddr + 2]; \n</code></pre> <ul> <li>The offline compiler can replicate the local memory</li> <li>This allows to create multiple ports </li> <li>Behaviour: <ul> <li>All read ports will be accessed in parallel </li> <li>All write ports are connected together</li> <li>Data between replicate is identical </li> </ul> </li> <li>Parallel access to all ports is possible but consumes more hardware resources</li> <li><code>[[intel::max_replicates(N)]]</code> control the replication factor</li> </ul> <p> </p>"},{"location":"writing/","title":"Developing SYCL programs for Intel\u00ae FPGA cards","text":""},{"location":"writing/#anatomy-of-a-sycl-program","title":"Anatomy of a SYCL program","text":""},{"location":"writing/#data-management","title":"Data Management","text":"<p>In the context of SYCL, Unified Shared Memory (USM) and buffers represent two different ways to handle memory and data management. They offer different levels of abstraction and ease of use, and the choice between them may depend on the specific needs of an application. Here's a breakdown of the differences:</p>"},{"location":"writing/#unified-shared-memory-usm","title":"Unified Shared Memory (USM)","text":"<p>Unified Shared Memory is a feature that simplifies memory management by providing a shared memory space across the host and various devices, like CPUs, GPUs, and FPGAs. USM provides three different types of allocations:</p> <ol> <li>Device Allocations: Allocated memory is accessible only by the device.</li> <li>Host Allocations: Allocated memory is accessible by the host and can be accessed by devices. However, the allocated memory is stored on the host global memory. </li> <li>Shared Allocations: Allocated memory is accessible by both the host and devices. The allocated memory is present in both global memories and it is synchronized between host and device.</li> </ol> <p>USM allows for more straightforward coding, akin to standard C++ memory management, and may lead to code that is easier to write and maintain. </p> <p>FPGA support</p> <p>SYCL USM host allocations are only supported by some BSPs, such as the Intel\u00ae FPGA Programmable Acceleration Card (PAC) D5005 (previously known as Intel\u00ae FPGA Programmable Acceleration Card (PAC) with Intel\u00ae Stratix\u00ae 10 SX FPGA).</p> <p>Using SYCL, you can verify if you have access to the different features:</p> <p>Verify USM capabilities</p> <pre><code>if (!device.has(sycl::aspect::usm_shared_allocations)) {\n    # Try to default to host allocation only\n    if (!device.has(sycl::aspect::usm_host_allocations)) {\n        # Default to device and explicit data movement\n        std::array&lt;int,N&gt; host_array;\n        int *my_array = malloc_device&lt;int&gt;(N, Q);\n    }else{\n        # Ok my_array is located on host memory but transferred to device as needed\n        int* my_array = malloc_host&lt;int&gt;(N, Q);\n    }\n}else{\n        # Ok my_array is located on both global memories and synchronized automatically \n        int* shared_array = malloc_shared&lt;int&gt;(N, Q);\n}\n</code></pre> <p>That's not all</p> <ul> <li>Concurrent accesses and atomic modifications are not necessarily available even if you have host and shared capabilities.</li> <li>You need to verify <code>aspect::usm_atomic_shared_allocations</code> and <code>aspect::usm_atomic_host_allocations</code>.</li> </ul> <p>Bittware 520N-MX</p> <p>The USM host allocations is not supported by some BSPs. We will therefore use explicit data movement.</p> <p>Explicit USM</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Replace the original code with explicit USM code </li> <li>Verify your code using emulation</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nvoid VectorAdd(const int *vec_a_in, const int *vec_b_in, int *vec_c_out,\n               int len) {\n  for (int idx = 0; idx &lt; len; idx++) {\n    int a_val = vec_a_in[idx];\n    int b_val = vec_b_in[idx];\n    int sum = a_val + b_val;\n    vec_c_out[idx] = sum;\n  }\n}\n\nconstexpr int kVectSize = 256;\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int host_vec_a[kVectSize];\n    int host_vec_b[kVectSize];\n    int host_vec_c[kVectSize];\n    int * vec_a = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_b = malloc_device&lt;int&gt;(kVectSize,q);\n    int * vec_c = malloc_device&lt;int&gt;(kVectSize,q);\n    for (int i = 0; i &lt; kVectSize; i++) {\n      host_vec_a[i] = i;\n      host_vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n\n    q.memcpy(vec_a, host_vec_a, kVectSize * sizeof(int)).wait();\n    q.memcpy(vec_b, host_vec_b, kVectSize * sizeof(int)).wait();\n\n\n\n    q.single_task&lt;VectorAddID&gt;([=]() {\n        VectorAdd(vec_a, vec_b, vec_c, kVectSize);\n      }).wait();\n\n    q.memcpy(host_vec_c, vec_c, kVectSize * sizeof(int)).wait();\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = host_vec_a[i] + host_vec_b[i];\n      if (host_vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; host_vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; host_vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; host_vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    sycl::free(vec_a,q);\n    sycl::free(vec_b,q);\n    sycl::free(vec_c,q);\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre>"},{"location":"writing/#buffer-accessors","title":"Buffer &amp; accessors","text":"<p>Buffers and accessors are key abstractions that enable memory management and data access across various types of devices like CPUs, GPUs, DSPs, etc.</p> <ol> <li> <p>Buffers:Buffers in SYCL are objects that represent a region of memory accessible by the runtime. They act as containers for data and provide a way to abstract the memory management across host and device memories. This allows for efficient data movement and optimization by the runtime, as it can manage the data movement between host and device memory transparently.</p> </li> <li> <p>Accessors:Accessors provide a way to access the data inside buffers. They define the type of access (read, write, read-write) and can be used within kernels to read from or write to buffers.</p> </li> </ol> <p>Advantage</p> <p>Through the utilization of these accessors, the SYCL runtime examines the interactions with the buffers and constructs a dependency graph that maps the relationship between host and device functions. This enables the runtime to automatically orchestrate the transfer of data and the sequencing of kernel activities.</p> <p>Using Buffers and Accessors</p> <pre><code>    #include &lt;array&gt; \n    // oneAPI headers\n    #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n    #include &lt;sycl/sycl.hpp&gt;\n\n    class Kernel;\n    constexpr int N = 100;\n    std::array&lt;int,N&gt; in_array;\n    std::array&lt;int,N&gt; out_array;\n    for (int i = 0 ; i &lt;N; i++)\n        in_array[i] = i+1;\n    queue device_queue(sycl::ext::intel::fpga_selector_v);\n\n    { // This one is very important to define the buffer scope\n      // buffer&lt;int, 1&gt; in_device_buf(in.data(), in.size());\n      // Or more convenient\n\n      buffer in_device_buf(in_array);\n      buffer out_device_buf(out_array);\n      device_queue.submit([&amp;](handler &amp;h) {\n        accessor in(in_device_buf, h, read_only);\n        accessor out(out_device_buf, h, write_only, no_init);\n        h.single_task&lt;Kernel&gt;([=]() { });\n      };\n    } \n    // Accessor going out of the scope\n    // Data has been copied back !!!\n</code></pre> <p>What about memory accesses in FPGA ? </p> <ul> <li>For FPGAs, the access pattern, access width, and coalescing of memory accesses can significantly affect performance. You might want to make use of various attributes and pragmas specific to your compiler and FPGA to guide the compiler in optimizing memory accesses.</li> <li>In order to use Direct Memory Access (DMA), you will need to setup proper data alignment or the offline compiler will output the following warnings: <pre><code>Running on device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nadd two vectors of size 256\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb60b350) and/or dev offset (0x400) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from host to device because of lack of alignment\n**                 host ptr (0xb611910) and/or dev offset (0x800) is not aligned to 64 bytes\n** WARNING: [aclbitt_s10mx_pcie0] NOT using DMA to transfer 1024 bytes from device to host because of lack of alignment\n**                 host ptr (0xb611d20) and/or dev offset (0xc00) is not aligned to 64 bytes\n</code></pre></li> <li>For example, you may need to replace: <pre><code>    int * vec_a = new int[kVectSize];\n    int * vec_b = new int[kVectSize];\n    int * vec_c = new int[kVectSize];\n</code></pre> by these ones: <pre><code>   int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n   int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize]; \n</code></pre></li> </ul>"},{"location":"writing/#queue","title":"Queue","text":"<p>Contrary to OpenCL, queues in SYCL are out-of-order by default. Nonetheless, you can change this behavior you declare it in your code.</p> <p>In-order-queue</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v,{property::queue::in_order()}};\n  // Task A\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  // Task B\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  device_queue.submit([&amp;](handler&amp; h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  }); \n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nB[TaskB] --&gt; C[TaskC];</code></pre> </p> <p>This behavior is not very useful nor flexible. Queue objects, by default, are out-of-order queues, except when they're constructed with the in-order queue property. Because of this, they must include mechanisms to arrange tasks that are sent to them. The way queues organize tasks is by allowing the user to notify the runtime about the dependencies that exist between these tasks. These dependencies can be described in two ways: either explicitly or implicitly, through the use of command groups.</p> <p>A command group is a specific object that outlines a task and its dependencies. These groups are generally expressed as C++ lambdas and are handed over as arguments to the submit() method within a queue object. The single parameter within this lambda is a reference to a handler object, utilized inside the command group to define actions, generate accessors, and outline dependencies.</p>"},{"location":"writing/#explicit-dependencies","title":"Explicit dependencies","text":"<p>Like for OpenCL, you can manage dependencies explicitly using events. </p> <p>Using events</p> <p> <pre><code>  ... \n  queue device_queue{sycl::ext::intel::fpga_selector_v};\n  // Task A\n  auto event_A = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskA&gt;([=]() { });\n  });\n  event_A.wait();\n  // Task B\n  auto event_B = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskB&gt;([=]() { });\n  });\n  // Task C\n  auto event_C = device_queue.submit([&amp;](handler &amp;h) {\n        h.single_task&lt;TaskC&gt;([=]() { });\n  });\n  // Task D\n  device_queue.submit([&amp;](handler &amp;h) {\n  h.depends_on({event_B, event_C});\n  h.parallel_for(N, [=](id&lt;1&gt; i) { /*...*/ });\n  }).wait();\n  ...\n</code></pre> <pre><code>graph TD\nA[TaskA] --&gt; B[TaskB];\nA[TaskA] --&gt; C[TaskC];\nB[TaskB] --&gt; D[TaskD];\nC[TaskC] --&gt; D[TaskD];</code></pre> </p> <ul> <li>Explicit dependencies using events is relevant when you use USM since buffers make use of accessors to model data dependencies.</li> <li>They are three possibilities to declare a dependcies explicitely:</li> <li>Calling the method <code>wait()</code> on the queue it-self</li> <li>Calling the method <code>wait</code> on the event return by the queue after submitting a command</li> <li>Calling the method <code>depends_on</code> of the handler object</li> </ul>"},{"location":"writing/#implicit-dependencies","title":"Implicit dependencies","text":"<ul> <li>Implicit dependencies occurs when your are using buffer &amp; accessor.</li> <li> <p>Accessors have different access modes:</p> </li> <li> <p>read_only: The content of the buffer can only be accessed for reading. So the content will only be copied once to the device</p> </li> <li>write_only: The content of the buffer can only be accessed for writing. The content of buffer is still copied from host to device before the kernel starts </li> <li>read_write: The content of the buffer can be accessed for reading and writing.</li> </ul> <p>You can add the <code>no_init</code> property to an accessor in <code>write_only</code> mode. This tells the runtime that the original data contains in the buffer can be ignored and don't need to be copied from host to device.</p> <p>Implicit dependencies obey to three main patterns (see DPC++ book):</p> <ul> <li>Read-after-Write  (RAW) : occurs when some data modified by a kernel should be read by another kernel. </li> <li>Write-after-Read  (WAR) : occurs when some data read by a kernel will be modified by another one</li> <li>Write-after-Write (WAW) : occurs when two kernels modified the same data</li> </ul> <p>Implicit dependencies</p> QuestionSolution <ul> <li>By default without access mode, each accessor will be read_write inducing unnecessary copies.</li> <li>Note also the first use of <code>host_accessor</code>. Why did we use it here ?</li> <li>Modifiy the following code to take into account implicit dependencies.  <pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h};\n      accessor aB{B, h};\n      accessor aC{C, h};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h};\n       accessor aB{B, h};\n       accessor aC{C, h};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C};\n</code></pre></li> </ul> <p><pre><code>   constexpr int N = 100;\n   queue Q;\n   buffer&lt;int&gt; A{range{N}};\n   buffer&lt;int&gt; B{range{N}};\n   buffer&lt;int&gt; C{range{N}};\n   Q.submit([&amp;](handler &amp;h) {\n      accessor aA{A, h, write_only, no_init};\n      accessor aB{B, h, write_only, no_init};\n      accessor aC{C, h, write_only, no_init};\n      h.single_task&lt;Kernel1&gt;([=]() { \n         for(unsigned int i =0; i&lt;N; i++)\n             aA[i] = 10;\n             aB[i] = 50;\n             aC[i] = 0;\n      });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aA{A, h, read_only};\n       accessor aB{B, h, read_only};\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel2&gt;([=]() { \n          for(unsigned int i =0; i&lt;N; i++)\n             aC[i] += aA[i] + aB[i]; \n        });\n   });\n   Q.submit([&amp;](handler &amp;h) {\n       accessor aC{C, h, write_only};\n       h.single_task&lt;Kernel3&gt;([=]() {\n         for(unsigned int i =0; i&lt;N; i++)\n            aC[i]++; \n       });\n   });\n   host_accessor result{C, read_only};\n</code></pre> * The <code>host_accessor</code> obtains access to buffer on the host and will wait for device kernel to execute to generate data.</p>"},{"location":"writing/#parallelism-model-for-fpga","title":"Parallelism model for FPGA","text":"<p>Vectorization</p> <p>Vectorization is not the main source of parallelism but help designing efficient pipeline. Since hardware can be reconfigured at will. The offline compiler can design N-bits Adders, multipliers which simplify greatly vectorization. In fact, the offline compiler vectorizes your design automatically if possible.</p>"},{"location":"writing/#pipelining-with-nd-range-kernels","title":"Pipelining with ND-range kernels","text":"<ul> <li>ND-range kernels are based on a hierachical grouping of work-items</li> <li>A work-item represents a single unit of work </li> <li>Independent simple units of work don't communicate or share data very often</li> <li>Useful when porting a GPU kernel to FPGA</li> </ul> DPC++ book -- Figure 17-15  <ul> <li>FPGAs are different from GPU (lots of threads started at the same time)</li> <li>Impossible to replicate a hardware for a million of work-items</li> <li>Work-items are injected into the pipeline</li> <li>A deep pipeline means lots of work-items executing different tasks in parallel</li> </ul> DPC++ book -- Figure 17-16  <ul> <li>In order to write basic data-parallel kernel, you will need to use the <code>parallel_for</code> method. Below is an example of simple data-parallel kernel. As you can notice it, there is no notion of groups nor sub-groups. </li> </ul> <p>Matrix addition</p> <pre><code>   constexpr int N = 2048;\n   constexpr int M = 1024;\n   queue.submit([&amp;](sycl::handler &amp;h) {\n     sycl::accessor acc_a{buffer_a, h, sycl::read_only};\n     sycl::accessor acc_b{buffer_b, h, sycl::read_only};\n     sycl::accessor acc_c{buffer_c, h, sycl::read_write, sycl::no_init};\n     h.parallel_for(range{N, M}, [=](sycl::id&lt;2&gt; idx) {\n      acc_c[idx] = acc_a[idx] + acc_b[idx];\n     });\n   });\n</code></pre> <p>Vector addition</p> QuestionSolution <ul> <li>Go to the <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code></li> <li>Adapt the <code>vector_add.cpp</code> single-task kernel to a basis data-parallel kernel</li> <li>Emulate to verify your design</li> </ul> <pre><code>#include &lt;iostream&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass VectorAddID;\n\nconstexpr int kVectSize = 256;\n\nint main() {\nbool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n    #if FPGA_SIMULATOR\n        auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n    #elif FPGA_HARDWARE\n        auto selector = sycl::ext::intel::fpga_selector_v;\n    #else  // #if FPGA_EMULATOR\n        auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n    #endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n    // declare arrays and fill them\n    int * vec_a = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_b = new(std::align_val_t{ 64 }) int[kVectSize];\n    int * vec_c = new(std::align_val_t{ 64 }) int[kVectSize];\n    for (int i = 0; i &lt; kVectSize; i++) {\n      vec_a[i] = i;\n      vec_b[i] = (kVectSize - i);\n    }\n\n    std::cout &lt;&lt; \"add two vectors of size \" &lt;&lt; kVectSize &lt;&lt; std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      sycl::buffer buffer_a{vec_a, sycl::range(kVectSize)};\n      sycl::buffer buffer_b{vec_b, sycl::range(kVectSize)};\n      sycl::buffer buffer_c{vec_c, sycl::range(kVectSize)};\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::write_only, sycl::no_init};\n\n        h.parallel_for&lt;VectorAddID&gt;(sycl::range(kVectSize),[=](sycl::id&lt;1&gt; idx) {\n      accessor_c[idx] = accessor_a[idx] + accessor_b[idx];\n        });\n      });\n    }\n    // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that VC is correct\n    for (int i = 0; i &lt; kVectSize; i++) {\n      int expected = vec_a[i] + vec_b[i];\n      if (vec_c[i] != expected) {\n        std::cout &lt;&lt; \"idx=\" &lt;&lt; i &lt;&lt; \": result \" &lt;&lt; vec_c[i] &lt;&lt; \", expected (\"\n                  &lt;&lt; expected &lt;&lt; \") A=\" &lt;&lt; vec_a[i] &lt;&lt; \" + B=\" &lt;&lt; vec_b[i]\n                  &lt;&lt; std::endl;\n        passed = false;\n      }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n    delete[] vec_a;\n    delete[] vec_b;\n    delete[] vec_c;\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <ul> <li>If you want to have a fine-grained control of your data-parallel kernel, ND-range data-parallel kernels are the equivalent of ND-range kernels in OpenCL. </li> </ul> <p>ND-range kernel in SYCL</p> <ul> <li><code>nd_range(range&lt;dimensions&gt; globalSize, range&lt;dimensions&gt; localSize);</code></li> <li>ND-range kernels are defined with two range objects<ul> <li>global representing the total size of work-items</li> <li>local representing the size of work-groups</li> </ul> </li> </ul> <p>Tiled Matrix Multiplication</p> QuestionSolution <ul> <li>Fill the blank and complete the code  <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    constexpr size_t N = 512;\n    constexpr size_t B =  16;\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      /* DEFINE HERE the global size and local size ranges*/\n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n        h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            // Provide local indexes i and j -- fill here\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              // Do the same for tileB\n              // fill here \n              item.barrier();\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n       sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n         // Fill here \n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n  // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n      for (int j = 0; j &lt; N; j++){\n         float true_val=0.0;\n         for (int k = 0 ; k &lt; N; k++){\n           true_val += mat_a[i*N +k] * mat_b[k*N+j];\n         }\n         if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n            std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n            passed = false;\n         }\n    }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre></li> </ul> <pre><code>#include &lt;iostream&gt;\n#include &lt;algorithm&gt;\n#include &lt;random&gt;\n\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n\n#include &lt;boost/align/aligned_allocator.hpp&gt;\n\n#define N 512\n#define B  16\n\n// Forward declare the kernel name in the global scope. This is an FPGA best\n// practice that reduces name mangling in the optimization reports.\nclass MatMultKernel;\n\n\nint main() {\n  bool passed = true;\n  try {\n    // Use compile-time macros to select either:\n    //  - the FPGA emulator device (CPU emulation of the FPGA)\n    //  - the FPGA device (a real FPGA)\n    //  - the simulator device\n#if FPGA_SIMULATOR\n    auto selector = sycl::ext::intel::fpga_simulator_selector_v;\n#elif FPGA_HARDWARE\n    auto selector = sycl::ext::intel::fpga_selector_v;\n#else  // #if FPGA_EMULATOR\n    auto selector = sycl::ext::intel::fpga_emulator_selector_v;\n#endif\n\n    // create the device queue\n    sycl::queue q(selector);\n\n    // make sure the device supports USM host allocations\n    auto device = q.get_device();\n\n    std::cout &lt;&lt; \"Running on device: \"\n              &lt;&lt; device.get_info&lt;sycl::info::device::name&gt;().c_str()\n              &lt;&lt; std::endl;\n\n\n    // initialize input and output memory on the host\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_a(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_b(N * N);\n    std::vector&lt;float,boost::alignment::aligned_allocator&lt;float,64&gt;&gt; mat_c(N * N); \n\n    std::random_device rd;\n    std::mt19937 mt(rd());\n    std::uniform_real_distribution&lt;float&gt; dist(0.0, 1.0);\n\n    // Generate random values\n    std::generate(mat_a.begin(), mat_a.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // Generate random values\n    std::generate(mat_b.begin(), mat_b.end(), [&amp;dist, &amp;mt]() {\n      return dist(mt);\n    });\n\n    // fill with zero\n    std::fill(mat_c.begin(), mat_c.end(), 0.0); \n\n\n    std::cout &lt;&lt; \"Matrix multiplication A X B = C \" &lt;&lt;std::endl;\n    {\n      // copy the input arrays to buffers to share with kernel\n      // We can access the buffer using mat[i][j]\n      sycl::buffer&lt;float,2&gt; buffer_a{mat_a.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_b{mat_b.data(), sycl::range&lt;2&gt;(N,N)};\n      sycl::buffer&lt;float,2&gt; buffer_c{mat_c.data(), sycl::range&lt;2&gt;(N,N)};\n\n\n      sycl::range global {N,N};\n      sycl::range local  {B,B}; \n\n\n      q.submit([&amp;](sycl::handler &amp;h) {\n        // use accessors to interact with buffers from device code\n        sycl::accessor accessor_a{buffer_a, h, sycl::read_only};\n        sycl::accessor accessor_b{buffer_b, h, sycl::read_only};\n        sycl::accessor accessor_c{buffer_c, h, sycl::read_write, sycl::no_init};\n\n        sycl::local_accessor&lt;float,2&gt; tileA{{B,B}, h};\n        sycl::local_accessor&lt;float,2&gt; tileB{{B,B}, h};\n\n       h.parallel_for&lt;MatMultKernel&gt;(sycl::nd_range{global, local}, [=](sycl::nd_item&lt;2&gt; item)\n\n            [[intel::max_work_group_size(1, B, B)]]    {\n            // Indices in the global index space:\n            int m = item.get_global_id()[0];\n            int n = item.get_global_id()[1];\n\n            // Index in the local index space:\n            int i = item.get_local_id()[0];\n           int j = item.get_local_id()[1];\n\n            float sum = 0;\n            for (int p = 0; p &lt; N/B; p++) {\n              // Load the matrix tile from matrix A, and synchronize\n              // to ensure all work-items have a consistent view\n              // of the matrix tile in local memory.\n              tileA[i][j] = accessor_a[m][p*B+j];\n              tileB[i][j] = accessor_b[p*B+i][n];\n              item.barrier();\n\n              // Perform computation using the local memory tile, and\n              // matrix B in global memory.\n              for (int kk = 0; kk &lt; B; kk++) {\n               sum += tileA[i][kk] * tileB[kk][j];\n              }\n\n              // After computation, synchronize again, to ensure all\n              // reads from the local memory tile are complete.\n              item.barrier();\n            }\n\n            // Write the final result to global memory.\n            accessor_c[m][n] = sum;\n\n        });\n      });\n    }\n\n\n  // result is copied back to host automatically when accessors go out of\n    // scope.\n\n    // verify that Matrix multiplication is correct\n    for (int i = 0; i &lt; N; i++) {\n   for (int j = 0; j &lt; N; j++){\n      float true_val=0.0;\n      for (int k = 0 ; k &lt; N; k++){\n       true_val += mat_a[i*N +k] * mat_b[k*N+j];\n      }\n          if (std::abs(true_val - mat_c[i*N+j])/true_val &gt; 1.0e-4 ) {\n               std::cout &lt;&lt; \"C[\" &lt;&lt; i &lt;&lt; \";\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; mat_c[i*N+j] &lt;&lt; \" expected = \" &lt;&lt; true_val &lt;&lt; std::endl;\n               passed = false;\n           }\n   }\n    }\n\n    std::cout &lt;&lt; (passed ? \"PASSED\" : \"FAILED\") &lt;&lt; std::endl;\n\n  } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n  return passed ? EXIT_SUCCESS : EXIT_FAILURE;\n}\n</code></pre> <p>Warning on work-items group size</p> <ul> <li>If the attribute [[intel::max_work_group_size(Z, Y, X)]] is not specified in your kernel, the workgroup size assumes a default value depending on compilation time and runtime constraints</li> <li>If your kernel contains a barrier, the Intel\u00ae oneAPI DPC++/C++ Compiler sets a default maximum scalarized work-group size of 128 work-items ==&gt; without this attribute, the previous ND-Range kernel would have failed since we have a local work-group size of B x B = 256 work-items </li> </ul>"},{"location":"writing/#pipelining-with-single-work-item-loop","title":"Pipelining with single-work item (loop)","text":"<ul> <li>When your code can't be decomposed into independent works, you can rely on loop parallelism using FPGA</li> <li>In such a situation, the pipeline inputs is not work-items but loop iterations</li> <li>For single-work-item kernels, the developer does not need to do anything special to preserve the data dependency </li> <li>Communications between kernels is also much easier</li> </ul> DPC++ book -- Figure 17-21  <ul> <li>FPGA can efficiently handle loop execution, often maintaining a fully occupied pipeline or providing reports on what changes are necessary to enhance occupancy.</li> <li>It's evident that if loop iterations were substituted with work-items, where the value created by one work-item would have to be transferred to another for incremental computation, the algorithm's description would become far more complex.</li> </ul> <p>Single-work item creation</p> <ul> <li>Replace the <code>parallel_for</code>method by the <code>single_task</code> method defined in the handler class to create a single-work item kernel</li> <li>The source file <code>vector_add.cpp</code> from <code>GettingStarted/fpga_compile/part4_dpcpp_lambda_buffers/src</code> uses loop pipelining.</li> </ul> <pre><code>  #include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n  #include &lt;sycl/sycl.hpp&gt;\n\n  using namespace sycl;\n\n  int main(){\n\n\n  // queue creation &amp; data initialization\n\n\n   q.submit([&amp;](handler &amp;h) {\n     h.single_task&lt;class MyKernel&gt;([=]() {\n       // Code to be executed as a single task\n     });\n   });\n   q.wait();\n  }\n</code></pre>"},{"location":"writing/#task-parallelism","title":"Task parallelism","text":"<p>Pipes function as a first-come, first-served buffer system, linking different parts of a design. The Intel\u00ae oneAPI DPC++/C++ Compiler offers various pipe types:</p> <ul> <li> <p>Host Pipes: These establish a connection between a host and a device.</p> </li> <li> <p>Inter-Kernel Pipes: These facilitate efficient and low-latency data transfer and synchronization between kernels. They enable kernels to interact directly using on-device FIFO buffers, which utilize FPGA memory. The Intel\u00ae oneAPI DPC++/C++ Compiler promotes simultaneous kernel operation. By employing inter-kernel pipes for data movement among concurrently running kernels, data can be transferred without waiting for a kernel to finish, enhancing your design's throughput.</p> </li> <li> <p>I/O Pipes: This is a one-way connection to the hardware, either as a source or sink, which can be linked to an FPGA board's input or output functionalities. Such functionalities could encompass network interfaces, PCIe\u00ae, cameras, or other data acquisition or processing tools and protocols.</p> </li> </ul>"},{"location":"writing/#inter-kernel-pipes","title":"Inter-Kernel Pipes","text":"<ul> <li>We will only focus on Inter-Kernel Pipes to leverage task parallelism</li> <li>As for OpenCL programming, pipes can be blocking or non-blocking</li> <li>For Intel\u00ae oneAPI with FPGA, you need to include FPGA extension: <pre><code>#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n</code></pre></li> </ul> <p>Pipe creation and usage</p> Blocking pipesNon-Blocking pipes <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i]); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <pre><code>// Using alias eases considerably their usage\nusing my_pipe = ext::intel::pipe&lt;      \n                class InterKernelPipe, // An identifier for the pipe.\n                int,                   // The type of data in the pipe.\n                4&gt;;                    // The capacity of the pipe.\n\n// Single_task kernel 1\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_in, h);\n    h.single_task([=]() {\n        valid_write = false;\n        for (int i=0; i &lt; count; i++) {\n            my_pipe::write(A[i],valid_write); // write a single int into the pipe\n\n        }\n    });\n}); \n\n// Single_task kernel 2\nq.submit([&amp;](handler&amp; h) {\n    auto A = accessor(B_out, h);\n    h.single_task([=]() {\n        valid_read = false;\n        for (int i=0; i &lt; count; i++) {\n            A[i] = my_pipe::read(valid_read); // read the next int from the pipe\n        }\n    });\n}); \n</code></pre> <p>Stalling pipes</p> <ul> <li>Care should be taken when implementing pipes, especially when there is a strong imbalance between the consumer kernel reading from the pipe and the producer kernel that feed the pipe. </li> <li>Stalling pipes can be disastrous when using blocking pipes</li> </ul>"},{"location":"writing/#multiple-homogeneous-fpga-devices","title":"Multiple Homogeneous FPGA Devices","text":"<ul> <li> <p>Each Meluxina's FPGA nodes have two FPGA cards</p> </li> <li> <p>You can verify their presence using the following commands: <code>aocl list-devices</code> or <code>sycl-ls</code> </p> </li> <li> <p>Differents kernels or the same kernels can be passed to these devices</p> </li> <li> <p>Each devices should have his own <code>sycl::queue</code> and share or not a same context</p> </li> <li> <p>Intel recommends to use a single context for performance reasons as show below:</p> </li> </ul> <p>Running on the two FPGA cards</p> <pre><code>    ...\n\n    sycl::platform p(selector);\n    auto devices = p.get_devices();\n    sycl::context C(devices);\n    sycl::queue q0 (C, devices[0]);\n    sycl::queue q1 (C, devices[1]);\n\n\n   std::cout &lt;&lt; \"Running on device: \"\n             &lt;&lt; devices[0].get_info&lt;sycl::info::device::name&gt;().c_str()\n             &lt;&lt; std::endl;\n\n   std::cout &lt;&lt; \"Running on device: \"\n             &lt;&lt; devices[1].get_info&lt;sycl::info::device::name&gt;().c_str()\n             &lt;&lt; std::endl;\n\n   ... \n</code></pre>"},{"location":"writing/#multiple-nodes","title":"Multiple nodes","text":"<ul> <li> <p>Meluxina FPGA's partition contains 20 nodes </p> </li> <li> <p>Combining MPI with the SYCL language allows developers to scale applications across diverse platforms within a distributed computing environment.</p> </li> <li> <p>Note that MPI cannot be called inside a kernel </p> </li> <li> <p>FPGA comminucation path :</p> </li> </ul> <pre><code>graph LR\n    A[FPGA 1] --&gt;|PCIe| B[NODE 1];\n    B[NODE 1] --&gt;|Infiniband| C[NODE 2];\n    C[NODE 1] --&gt;|PCIe| D[FPGA 2];</code></pre> <p>MPI Programs Using C++ with SYCL running on  multiple FPGAs</p> <p><pre><code>#include &lt;mpi.h&gt;\n// oneAPI headers\n#include &lt;sycl/ext/intel/fpga_extensions.hpp&gt;\n#include &lt;sycl/sycl.hpp&gt;\n#include &lt;iomanip&gt;  // setprecision library\n#include &lt;iostream&gt;\n#include &lt;numeric&gt; \n\n\nusing namespace sycl;\nconstexpr int master = 0;\n\n////////////////////////////////////////////////////////////////////////\n//\n// Each MPI ranks compute the number Pi partially on target device using SYCL.\n// The partial result of number Pi is returned in \"results\".\n//\n////////////////////////////////////////////////////////////////////////\nvoid mpi_native(double* results, int rank_num, int num_procs,\n                long total_num_steps, queue&amp; q) {\n\n  double dx = 1.0f / (double)total_num_steps;\n  long items_per_proc = total_num_steps / size_t(num_procs);\n  // The size of amount of memory that will be given to the buffer.\n  //range&lt;1&gt; num_items{items_per_proc};\n\n  // Buffers are used to tell SYCL which data will be shared between the host\n  // and the devices.\n  buffer&lt;double, 1&gt; results_buf(results,\n                               range&lt;1&gt;(items_per_proc));\n\n  // Submit takes in a lambda that is passed in a command group handler\n  // constructed at runtime.\n  q.submit([&amp;](handler&amp; h) {\n    // Accessors are used to get access to the memory owned by the buffers.\n    accessor results_accessor(results_buf,h,write_only);\n    // Each kernel calculates a partial of the number Pi in parallel.\n    h.parallel_for(range&lt;1&gt;(items_per_proc), [=](id&lt;1&gt; k) {\n      double x = ((double)(rank_num * items_per_proc + k))  * dx ;\n      results_accessor[k] = (4.0f * dx) / (1.0f + x * x);\n    });\n  });\n}\n\n\nint main(int argc, char** argv) {\n  long num_steps = 1000000;\n  char machine_name[MPI_MAX_PROCESSOR_NAME];\n  int name_len=0;\n  int id=0;\n  int num_procs=0;\n  double pi=0.0;\n  double t1, t2;\n  try {\n  // Use compile-time macros to select either:\n  //   - the FPGA emulator device (CPU emulation of the FPGA)\n  //   - the FPGA device (a real FPGA)\n  //   - the simulator device\n  #if FPGA_SIMULATOR\n  auto selector = ext::intel::fpga_simulator_selector_v;\n  #elif FPGA_HARDWARE\n  auto selector = ext::intel::fpga_selector_v;\n  #elif FPGA_EMULATOR\n  auto selector = ext::intel::fpga_emulator_selector_v;\n  #else \n  auto selector = sycl::cpu_selector_v;\n  #endif\n\n  property_list q_prop{property::queue::in_order()};\n  queue myQueue{selector,q_prop};\n\n  // Start MPI.\n  if (MPI_Init(&amp;argc, &amp;argv) != MPI_SUCCESS) {\n    std::cout &lt;&lt; \"Failed to initialize MPI\\n\";\n    exit(-1);\n  }\n\n  // Create the communicator, and retrieve the number of MPI ranks.\n  MPI_Comm_size(MPI_COMM_WORLD, &amp;num_procs);\n\n  // Determine the rank number.\n  MPI_Comm_rank(MPI_COMM_WORLD, &amp;id);\n\n  // Get the machine name.\n  MPI_Get_processor_name(machine_name, &amp;name_len);\n\n  if(id == master) t1 = MPI_Wtime();\n\n  std::cout &lt;&lt; \"Rank #\" &lt;&lt; id &lt;&lt; \" runs on: \" &lt;&lt; machine_name\n            &lt;&lt; \", uses device: \"\n            &lt;&lt; myQueue.get_device().get_info&lt;info::device::name&gt;() &lt;&lt; \"\\n\";\n\n  int num_step_per_rank = num_steps / num_procs;\n  double* results_per_rank = new double[num_step_per_rank];\n\n  // Initialize an array to store a partial result per rank.\n  for (size_t i = 0; i &lt; num_step_per_rank; i++) results_per_rank[i] = 0.0;\n\n  // Calculate the Pi number partially by multiple MPI ranks.\n  mpi_native(results_per_rank, id, num_procs, num_steps, myQueue);\n\n  double local_sum = 0.0;\n  for(unsigned int i = 0; i &lt; num_step_per_rank; i++){\n    local_sum += results_per_rank[i];\n  }\n\n  // Master rank performs a reduce operation to get the sum of all partial Pi.\n  MPI_Reduce(&amp;local_sum, &amp;pi, 1, MPI_DOUBLE, MPI_SUM, master, MPI_COMM_WORLD);\n\n  if (id == master) {\n    t2 = MPI_Wtime(); \n    std::cout &lt;&lt; \"mpi native:\\t\\t\";\n    std::cout &lt;&lt; std::setprecision(10) &lt;&lt; \"PI =\" &lt;&lt; pi &lt;&lt; std::endl;\n    std::cout &lt;&lt; \"Elapsed time is \" &lt;&lt; t2-t1 &lt;&lt; std::endl;\n  }\n\n  delete[] results_per_rank;\n\n  MPI_Finalize();\n\n } catch (sycl::exception const &amp;e) {\n    // Catches exceptions in the host code.\n    std::cerr &lt;&lt; \"Caught a SYCL host exception:\\n\" &lt;&lt; e.what() &lt;&lt; \"\\n\";\n\n    // Most likely the runtime couldn't find FPGA hardware!\n    if (e.code().value() == CL_DEVICE_NOT_FOUND) {\n      std::cerr &lt;&lt; \"If you are targeting an FPGA, please ensure that your \"\n                   \"system has a correctly configured FPGA board.\\n\";\n      std::cerr &lt;&lt; \"Run sys_check in the oneAPI root directory to verify.\\n\";\n      std::cerr &lt;&lt; \"If you are targeting the FPGA emulator, compile with \"\n                   \"-DFPGA_EMULATOR.\\n\";\n    }\n    std::terminate();\n  }\n\n  return 0;\n}\n</code></pre>  Output : <pre><code>Rank #3 runs on: mel3014, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #0 runs on: mel3001, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #4 runs on: mel3017, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #2 runs on: mel3013, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nRank #1 runs on: mel3010, uses device: p520_hpc_m210h_g3x16 : BittWare Stratix 10 MX OpenCL platform (aclbitt_s10mx_pcie0)\nmpi native:             PI =3.141593654\nElapsed time is 9.703053059\n</code></pre></p>"}]}